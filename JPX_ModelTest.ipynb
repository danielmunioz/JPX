{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JPX_ModelTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIbh63gyeeQG"
      },
      "outputs": [],
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "\n",
        "!rm -r sample_data\n",
        "!kaggle competitions download -c jpx-tokyo-stock-exchange-prediction\n",
        "!unzip ./jpx-tokyo-stock-exchange-prediction.zip -d jpx-tokyo-stock-exchange-prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5rAq4kJLep0K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSDataset(Dataset):\n",
        "  def __init__(self, df, seq_len=128, padding_token=0):\n",
        "    self.df = df\n",
        "    self.indices = []\n",
        "    self.seq_len = seq_len\n",
        "    self.padding_token = padding_token\n",
        "    \n",
        "    #Creating indices\n",
        "    start = 0\n",
        "    for _ in range(-(len(self.df) // -self.seq_len)):\n",
        "      self.indices.append((start, start+self.seq_len))\n",
        "      start+=self.seq_len\n",
        "    \n",
        "    #fixing non-perfect intervals, --in place\n",
        "    idx = 0\n",
        "    while idx<len(self.indices):\n",
        "      start, end = self.indices[idx]\n",
        "      intervals = self.df[start:end]['SecuritiesCode'].value_counts(sort=False).values\n",
        "      if len(intervals) != 1:\n",
        "        self.indices = self.indices[:idx] + [(start, start+intervals[0]), (start+intervals[0], end)] + self.indices[idx+1:]\n",
        "        idx+=1\n",
        "      idx+=1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    start, end = self.indices[idx]\n",
        "\n",
        "    target = self.df[start:end]['Target'].values[-1]\n",
        "    sequence = np.expand_dims(self.df[start:end]['Close'].values, 1)\n",
        "    if sequence.shape[0] != self.seq_len:\n",
        "      sequence = np.pad(sequence, pad_width=[(0, self.seq_len-sequence.shape[0]), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "    \n",
        "    return sequence, target"
      ],
      "metadata": {
        "id": "jrgh8BkAdvzY"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class time2vec(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.w_linear = nn.Parameter(data=torch.rand(in_features, 1))\n",
        "    self.b_linear = nn.Parameter(data=torch.rand(1))\n",
        "    self.w_function = nn.Parameter(data=torch.rand(in_features, out_features-1))\n",
        "    self.b_function = nn.Parameter(data=torch.rand(out_features-1))\n",
        "\n",
        "    #maybe a bit more straightforward\n",
        "    #self.linear_params = nn.Linear(in_features, 1, bias=True)\n",
        "    #self.function_params = nn.Linear(in_features, out_features-1, bias=True)\n",
        "\n",
        "    #initialize params?\n",
        "    #nn.init.kaiming_normal_(self.w_linear)\n",
        "    #nn.init.kaiming_normal_(self.b_linear)\n",
        "    #nn.init.kaiming_normal_(self.w_function)\n",
        "    #nn.init.kaiming_normal_(self.b_function)\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear_out = torch.matmul(x, self.w_linear)+self.b_linear\n",
        "    func_out = torch.sin(torch.matmul(x, self.w_function)+self.b_function)\n",
        "    return torch.concat((linear_out, func_out), dim=-1)"
      ],
      "metadata": {
        "id": "YtoKQLCyJSC4"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTransformer(nn.Module):\n",
        "  def __init__(self, in_features, time_features=1, mlp_dim=1024, enc_layers=2, enc_heads=2):\n",
        "    super().__init__()\n",
        "    self.time2vec = time2vec(in_features, time_features)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=in_features+time_features, nhead=enc_heads, \n",
        "                                                    dropout=0, activation=F.gelu, batch_first=True, norm_first=False)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=enc_layers)\n",
        "\n",
        "    self.mlp = nn.Linear(in_features+time_features, mlp_dim)\n",
        "    self.regressor = nn.Linear(mlp_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    time_embeddings = self.time2vec(x)\n",
        "    x = torch.concat((x, time_embeddings), dim=-1)\n",
        "    x = self.encoder(x)\n",
        "\n",
        "    x = F.relu(self.mlp(x))\n",
        "    x = self.regressor(x)\n",
        "\n",
        "    return x[:, -1, :] #returning only last seq element"
      ],
      "metadata": {
        "id": "hEAvaGbUJVZw"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "\n",
        "padding_token = 0.0\n",
        "missing_token = -1.0\n",
        "\n",
        "\n",
        "dframe = pd.read_csv('jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "stock_list = dframe.SecuritiesCode.unique()\n",
        "dframe_1 = dframe.drop(['Open', 'High', 'Low', 'Volume', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'], axis=1)\n",
        "dframe_1 = dframe_1[~dframe_1['Close'].isnull()] #Getting rid of null values for this experiment\n",
        "dframe_1 = dframe_1.sort_values(['SecuritiesCode', 'Date'], ascending=[True, True]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WGnujAZM0RNZ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Testing dataset ideas ----------\n",
        "start = 0\n",
        "indices = []\n",
        "for _ in range(-(len(dframe_1) // -seq_len)):\n",
        "  indices.append((start, start+seq_len))\n",
        "  start+= seq_len\n",
        "\n",
        "\n",
        "#Creates new idx array\n",
        "new_idxs = []\n",
        "for pos, elemen in enumerate(indices):\n",
        "  start, end = elemen\n",
        "  intervals = dframe_1[start:end]['SecuritiesCode'].value_counts(sort=False).values #False keeps the OG order(according to github)\n",
        "  if len(intervals) != 1:\n",
        "    new_idxs.extend([(start, start+intervals[0]), (start+intervals[0], end)])\n",
        "  else:\n",
        "    new_idxs.append((start, end))\n",
        "\n",
        "#Works in place\n",
        "idx = 0\n",
        "while idx<len(indices):\n",
        "  start, end = indices[idx]\n",
        "  intervals = dframe_1[start:end]['SecuritiesCode'].value_counts(sort=False).values\n",
        "  if len(intervals)!=1:\n",
        "    indices = indices[:idx] + [(start, start+intervals[0]), (start+intervals[0], end)] + indices[idx+1:]\n",
        "    idx+=1\n",
        "  idx+=1"
      ],
      "metadata": {
        "id": "uVT_0TdedhyG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = TSDataset(dframe_1, seq_len=128)\n",
        "dloader = DataLoader(dset, batch_size=64, shuffle=False, num_workers=1)\n",
        "\n",
        "train_batch, target_batch = next(iter(dloader))\n",
        "train_batch.shape, target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJua_g7qN5_p",
        "outputId": "278d2efc-6eab-4081-fdc4-f032aaea049f"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 128, 1]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Overfitting a single batch\n",
        "model = TSTransformer(in_features = 1)\n",
        "\n",
        "crit = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "for epoch in range(10):\n",
        "  out = model(train_batch.float())\n",
        "  loss = crit(out.squeeze(-1), target_batch.float())\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC3he-P2WjBE",
        "outputId": "a62ea3d4-3089-47fd-bfb2-593318012b1c"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0039, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
        "\n",
        "for epoch in range(10):\n",
        "  out = model(train_batch.float())\n",
        "  loss = crit(out.squeeze(-1), target_batch.float())\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJuc9G36cHfj",
        "outputId": "bd9567a2-225a-4837-d842-877eabdf6f03"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0003, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.squeeze(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fet_1xWce4S",
        "outputId": "94227258-5c8c-4f50-daca-0035f3ef71fb"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029,\n",
              "        -0.0029,  0.0002,  0.0002, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029,\n",
              "        -0.0029, -0.0029, -0.0029,  0.0002,  0.0002, -0.0029, -0.0029, -0.0029,\n",
              "        -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029,  0.0002,  0.0002,\n",
              "        -0.0029,  0.0002,  0.0002, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029,\n",
              "        -0.0029, -0.0029, -0.0029,  0.0002,  0.0002, -0.0029, -0.0029, -0.0029,\n",
              "        -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029,  0.0002,  0.0002,\n",
              "        -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029, -0.0029],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E16JKG2qcv-r",
        "outputId": "e89ee501-428a-4924-a0ea-c7735dc2a3db"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0096,  0.0144,  0.0056,  0.0132, -0.0142, -0.0362,  0.0033,  0.0031,\n",
              "         0.0000,  0.0316, -0.0092, -0.0269,  0.0124,  0.0028, -0.0154, -0.0454,\n",
              "        -0.0209, -0.0048,  0.0186, -0.0560,  0.0108, -0.0032,  0.0079, -0.0066,\n",
              "         0.0025,  0.0246, -0.0013, -0.0124,  0.0142, -0.0475,  0.0135,  0.0027,\n",
              "         0.0103,  0.0327,  0.0029,  0.0006, -0.0150, -0.0042,  0.0145,  0.0020,\n",
              "        -0.0097, -0.0044, -0.0067,  0.0326,  0.0140, -0.0015,  0.0193,  0.0028,\n",
              "        -0.0052,  0.0040,  0.0045,  0.0188,  0.0000, -0.0087,  0.0266,  0.0010,\n",
              "         0.0129,  0.0000, -0.0144,  0.0038, -0.0010,  0.0155, -0.0055,  0.0010],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IfxiWlv1cyVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}