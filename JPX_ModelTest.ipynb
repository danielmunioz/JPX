{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JPX_ModelTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "\n",
        "!rm -r sample_data\n",
        "!kaggle competitions download -c jpx-tokyo-stock-exchange-prediction\n",
        "!unzip ./jpx-tokyo-stock-exchange-prediction.zip -d jpx-tokyo-stock-exchange-prediction"
      ],
      "metadata": {
        "id": "GHRbYCxEWZIO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5rAq4kJLep0K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSDataset(Dataset):\n",
        "  def __init__(self, df, seq_len=128, padding_token=0):\n",
        "    self.df = df\n",
        "    self.indices = []\n",
        "    self.seq_len = seq_len\n",
        "    self.padding_token = padding_token\n",
        "    \n",
        "    #Creating indices\n",
        "    start = 0\n",
        "    for _ in range(-(len(self.df) // -self.seq_len)):\n",
        "      self.indices.append((start, start+self.seq_len))\n",
        "      start+=self.seq_len\n",
        "    \n",
        "    #fixing non-perfect intervals, --in place\n",
        "    idx = 0\n",
        "    while idx<len(self.indices):\n",
        "      start, end = self.indices[idx]\n",
        "      intervals = self.df[start:end]['SecuritiesCode'].value_counts(sort=False).values\n",
        "      if len(intervals) != 1:\n",
        "        self.indices = self.indices[:idx] + [(start, start+intervals[0]), (start+intervals[0], end)] + self.indices[idx+1:]\n",
        "        idx+=1\n",
        "      idx+=1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    start, end = self.indices[idx]\n",
        "\n",
        "    target = self.df[start:end]['Target'].values[-1]\n",
        "    sequence = np.expand_dims(self.df[start:end]['Close'].values, 1)\n",
        "    if sequence.shape[0] != self.seq_len:\n",
        "     sequence = np.pad(sequence, pad_width=[(self.seq_len-sequence.shape[0], 0), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "\n",
        "    #careful here padding_mask shape shouldn't be the same as sequence's\n",
        "    padding_mask = (sequence == self.padding_token)\n",
        "    \n",
        "    return {'sequence':sequence,\n",
        "            'mask':padding_mask,\n",
        "            'target':target}"
      ],
      "metadata": {
        "id": "jrgh8BkAdvzY"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class time2vec(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.w_linear = nn.Parameter(data=torch.rand(in_features, 1))\n",
        "    self.b_linear = nn.Parameter(data=torch.rand(1))\n",
        "    self.w_function = nn.Parameter(data=torch.rand(in_features, out_features-1))\n",
        "    self.b_function = nn.Parameter(data=torch.rand(out_features-1))\n",
        "\n",
        "    #maybe a bit more straightforward\n",
        "    #self.linear_params = nn.Linear(in_features, 1, bias=True)\n",
        "    #self.function_params = nn.Linear(in_features, out_features-1, bias=True)\n",
        "\n",
        "    #initialize params?\n",
        "    #nn.init.kaiming_normal_(self.w_linear)\n",
        "    #nn.init.kaiming_normal_(self.b_linear)\n",
        "    #nn.init.kaiming_normal_(self.w_function)\n",
        "    #nn.init.kaiming_normal_(self.b_function)\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear_out = torch.matmul(x, self.w_linear)+self.b_linear\n",
        "    func_out = torch.sin(torch.matmul(x, self.w_function)+self.b_function)\n",
        "    return torch.concat((linear_out, func_out), dim=-1)"
      ],
      "metadata": {
        "id": "YtoKQLCyJSC4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTransformer(nn.Module):\n",
        "  def __init__(self, in_features, time_features=1, mlp_dim=1024, enc_layers=2, enc_heads=2):\n",
        "    super().__init__()\n",
        "    self.time2vec = time2vec(in_features, time_features)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=in_features+time_features, nhead=enc_heads, \n",
        "                                                    dropout=0, activation=F.gelu, batch_first=True, \n",
        "                                                    norm_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=enc_layers)\n",
        "\n",
        "    self.mlp = nn.Linear(in_features+time_features, mlp_dim)\n",
        "    self.regressor = nn.Linear(mlp_dim, 1)\n",
        "\n",
        "  def forward(self, x, padding_mask):\n",
        "    time_embeddings = self.time2vec(x)\n",
        "    x = torch.concat((x, time_embeddings), dim=-1)\n",
        "    x = self.encoder(src=x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "    x = F.relu(self.mlp(x))\n",
        "    x = self.regressor(x)\n",
        "\n",
        "    return x[:, -1, :] #returning only last seq element"
      ],
      "metadata": {
        "id": "hEAvaGbUJVZw"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "\n",
        "padding_token = 0.0\n",
        "missing_token = -1.0\n",
        "\n",
        "\n",
        "dframe = pd.read_csv('jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "stock_list = dframe.SecuritiesCode.unique()\n",
        "dframe_1 = dframe.drop(['Open', 'High', 'Low', 'Volume', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'], axis=1)\n",
        "dframe_1 = dframe_1[~dframe_1['Close'].isnull()] #Getting rid of null values for this experiment\n",
        "dframe_1 = dframe_1.sort_values(['SecuritiesCode', 'Date'], ascending=[True, True]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WGnujAZM0RNZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = TSDataset(dframe_1, seq_len=128)\n",
        "dloader = DataLoader(dset, batch_size=128, shuffle=False, num_workers=1)\n",
        "\n",
        "train_batch = next(iter(dloader))\n",
        "train_batch['sequence'].shape, train_batch['mask'].shape, train_batch['target'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi2IBurw1e_4",
        "outputId": "0a9fb2f2-8d6b-4618-fa0d-5bc8211e686f"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 128, 1]), torch.Size([128, 128, 1]), torch.Size([128]))"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####Overfitting a single batch\n",
        "\n",
        "#May be a good idea to normalize the data\n",
        "#consider another criterion\n",
        "#consider modifying model params\n",
        "#consider encoding the vector dates instead of the sequence\n",
        "model = TSTransformer(in_features=1)\n",
        "crit = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "for epoch in range(10):\n",
        "  out = model(train_batch['sequence'].float(), padding_mask=train_batch['mask'].squeeze(-1).float())\n",
        "  loss = crit(out.squeeze(-1), train_batch['target'].float())\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0cKwsFUDevl",
        "outputId": "e2716373-27d9-484e-bed4-414a58761760"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(958358.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(848255.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(748055.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(654654.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(567155.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(485306.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(409115.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(338707.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(274258.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(215993.0312, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr = 1e-9)\n",
        "for epoch in range(5):\n",
        "  out = model(train_batch['sequence'].float(), padding_mask=train_batch['mask'].squeeze(-1).float())\n",
        "  loss = crit(out.squeeze(-1), train_batch['target'].float())\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaJCNER4IdI6",
        "outputId": "a86dee96-0b60-43c7-8bd1-c17e7ed2f4b5"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.squeeze(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fet_1xWce4S",
        "outputId": "c3c1838d-dcbd-4a95-b352-294f888284ae"
      },
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0198, -0.0041, -0.0135, -0.0249, -0.0224, -0.0251, -0.0256, -0.0186,\n",
              "        -0.0211, -0.0842, -0.0843, -0.0449, -0.0438, -0.0449, -0.0442, -0.0443,\n",
              "        -0.0431, -0.0426, -0.0434, -0.0778, -0.0824, -0.0200, -0.0188, -0.0090,\n",
              "        -0.0070, -0.0244, -0.0314, -0.0322, -0.0287, -0.0267, -0.0958, -0.0741,\n",
              "        -0.0396, -0.0938, -0.0938, -0.0402, -0.0395, -0.0425, -0.0431, -0.0414,\n",
              "        -0.0426, -0.0409, -0.0419, -0.0776, -0.0833, -0.0169, -0.0143, -0.0145,\n",
              "        -0.0094, -0.0107, -0.0195, -0.0106, -0.0096, -0.0154, -0.0840, -0.0803,\n",
              "        -0.0348, -0.0346, -0.0349, -0.0372, -0.0354, -0.0336, -0.0327, -0.0359,\n",
              "        -0.0914, -0.0951, -0.0153,  0.0022, -0.0231, -0.0304, -0.0290, -0.0286,\n",
              "        -0.0198, -0.0154, -0.0275, -0.0952, -0.0459, -0.0452, -0.0463, -0.0455,\n",
              "        -0.0460, -0.0444, -0.0243, -0.0188,  0.0088, -0.0409, -0.0870, -0.0244,\n",
              "        -0.0240, -0.0309, -0.0364, -0.0385, -0.0354, -0.0283, -0.0315, -0.0958,\n",
              "        -0.0447,  0.0374,  0.0415,  0.0531,  0.0443,  0.0001,  0.0032,  0.0024,\n",
              "         0.0022, -0.0432, -0.0951, -0.0442, -0.0398, -0.0383, -0.0406, -0.0402,\n",
              "        -0.0443, -0.0415, -0.0381, -0.0339, -0.0998, -0.0644, -0.0456, -0.0467,\n",
              "        -0.0453, -0.0417, -0.0425, -0.0451, -0.0373, -0.0260, -0.0916, -0.0855],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch['target'].float()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E16JKG2qcv-r",
        "outputId": "487c4422-c428-474c-a4d3-24121e935501"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0096,  0.0144,  0.0056,  0.0132, -0.0142, -0.0362,  0.0033,  0.0031,\n",
              "         0.0000,  0.0316, -0.0092, -0.0269,  0.0124,  0.0028, -0.0154, -0.0454,\n",
              "        -0.0209, -0.0048,  0.0186, -0.0560,  0.0108, -0.0032,  0.0079, -0.0066,\n",
              "         0.0025,  0.0246, -0.0013, -0.0124,  0.0142, -0.0475,  0.0135,  0.0027,\n",
              "         0.0103,  0.0327,  0.0029,  0.0006, -0.0150, -0.0042,  0.0145,  0.0020,\n",
              "        -0.0097, -0.0044, -0.0067,  0.0326,  0.0140, -0.0015,  0.0193,  0.0028,\n",
              "        -0.0052,  0.0040,  0.0045,  0.0188,  0.0000, -0.0087,  0.0266,  0.0010,\n",
              "         0.0129,  0.0000, -0.0144,  0.0038, -0.0010,  0.0155, -0.0055,  0.0010,\n",
              "         0.0191,  0.0055,  0.0000, -0.0086,  0.0295,  0.0326, -0.0125,  0.0024,\n",
              "        -0.0078, -0.0088,  0.0048, -0.0195, -0.0025,  0.0000, -0.0101,  0.0461,\n",
              "        -0.0059,  0.0182, -0.0061, -0.0031, -0.0226,  0.0193,  0.0069, -0.0211,\n",
              "        -0.0200,  0.0031, -0.0082,  0.0105,  0.0020, -0.0085, -0.0144,  0.0274,\n",
              "         0.0039, -0.0379, -0.0119,  0.0094, -0.0140,  0.0022,  0.0144, -0.0329,\n",
              "        -0.0032,  0.0158,  0.0000, -0.0144,  0.0067,  0.0273,  0.0000, -0.0055,\n",
              "         0.0114,  0.0135,  0.0157, -0.0068,  0.0156, -0.0031,  0.0269,  0.0346,\n",
              "         0.0066,  0.0298, -0.0257, -0.0095, -0.0032,  0.0432,  0.0293,  0.0053])"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IfxiWlv1cyVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}