{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JPX_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "\n",
        "!rm -r sample_data\n",
        "!kaggle competitions download -c jpx-tokyo-stock-exchange-prediction\n",
        "!unzip ./jpx-tokyo-stock-exchange-prediction.zip -d jpx-tokyo-stock-exchange-prediction"
      ],
      "metadata": {
        "id": "GHRbYCxEWZIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5rAq4kJLep0K"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSDataset(Dataset):\n",
        "  def __init__(self, df, seq_len=128, padding_token=0):\n",
        "    self.df = df\n",
        "    self.indices = []\n",
        "    self.seq_len = seq_len\n",
        "    self.padding_token = padding_token\n",
        "    \n",
        "    #Creating indices\n",
        "    start = 0\n",
        "    for _ in range(-(len(self.df) // -self.seq_len)):\n",
        "      self.indices.append((start, start+self.seq_len))\n",
        "      start+=self.seq_len\n",
        "    \n",
        "    #fixing non-perfect intervals, --in place\n",
        "    idx = 0\n",
        "    while idx<len(self.indices):\n",
        "      start, end = self.indices[idx]\n",
        "      intervals = self.df[start:end]['SecuritiesCode'].value_counts(sort=False).values\n",
        "      if len(intervals) != 1:\n",
        "        self.indices = self.indices[:idx] + [(start, start+intervals[0]), (start+intervals[0], end)] + self.indices[idx+1:]\n",
        "        idx+=1\n",
        "      idx+=1\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    start, end = self.indices[idx]\n",
        "    seq_df = self.df[start:end]\n",
        "\n",
        "    date_vec = np.concatenate([np.expand_dims(seq_df['Date'].dt.year.values, 1), \n",
        "                               np.expand_dims(seq_df['Date'].dt.month.values, 1), \n",
        "                               np.expand_dims(seq_df['Date'].dt.day.values, 1)], \n",
        "                              axis=1)\n",
        "    date_vec = np.pad(date_vec, pad_width=[(self.seq_len-date_vec.shape[0], 0), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "    \n",
        "    target = seq_df['Target'].values[-1]\n",
        "    sequence = np.expand_dims(seq_df['Close'].values, 1)\n",
        "    if sequence.shape[0] != self.seq_len:\n",
        "     sequence = np.pad(sequence, pad_width=[(self.seq_len-sequence.shape[0], 0), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "\n",
        "    #careful here padding_mask shape shouldn't be the same as sequence's, it works now bc we're using only one feature\n",
        "    padding_mask = (sequence == self.padding_token)\n",
        "    \n",
        "    return {'sequence':sequence,\n",
        "            'date':date_vec,\n",
        "            'mask':padding_mask,\n",
        "            'target':target}"
      ],
      "metadata": {
        "id": "jrgh8BkAdvzY"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class time2vec(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.w_linear = nn.Parameter(data=torch.rand(in_features, 1))\n",
        "    self.b_linear = nn.Parameter(data=torch.rand(1))\n",
        "    self.w_function = nn.Parameter(data=torch.rand(in_features, out_features-1))\n",
        "    self.b_function = nn.Parameter(data=torch.rand(out_features-1))\n",
        "\n",
        "    #maybe a bit more straightforward\n",
        "    #self.linear_params = nn.Linear(in_features, 1, bias=True)\n",
        "    #self.function_params = nn.Linear(in_features, out_features-1, bias=True)\n",
        "\n",
        "    #initialize params?\n",
        "    #nn.init.kaiming_normal_(self.w_linear)\n",
        "    #nn.init.kaiming_normal_(self.b_linear)\n",
        "    #nn.init.kaiming_normal_(self.w_function)\n",
        "    #nn.init.kaiming_normal_(self.b_function)\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear_out = torch.matmul(x, self.w_linear)+self.b_linear\n",
        "    func_out = torch.sin(torch.matmul(x, self.w_function)+self.b_function)\n",
        "    return torch.concat((linear_out, func_out), dim=-1)"
      ],
      "metadata": {
        "id": "YtoKQLCyJSC4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTransformer(nn.Module):\n",
        "  def __init__(self, in_features, time_features=7, mlp_dim=1024, enc_layers=2, enc_heads=2):\n",
        "    super().__init__()\n",
        "    self.time2vec = time2vec(3, time_features)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=in_features+time_features, nhead=enc_heads, \n",
        "                                                    dropout=0, activation=F.gelu, batch_first=True, \n",
        "                                                    norm_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=enc_layers)\n",
        "\n",
        "    self.mlp = nn.Linear(in_features+time_features, mlp_dim)\n",
        "    self.regressor = nn.Linear(mlp_dim, 1)\n",
        "\n",
        "  def forward(self, x, date_vec, padding_mask):\n",
        "    time_embeddings = self.time2vec(date_vec)\n",
        "    x = torch.concat((x, time_embeddings), dim=-1)\n",
        "    x = self.encoder(src=x, src_key_padding_mask=padding_mask)\n",
        "\n",
        "    x = F.relu(self.mlp(x))\n",
        "    x = self.regressor(x)\n",
        "\n",
        "    return x[:, -1, :] #returning only last seq element"
      ],
      "metadata": {
        "id": "hEAvaGbUJVZw"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "\n",
        "padding_token = 0.0\n",
        "missing_token = -1.0\n",
        "\n",
        "\n",
        "dframe = pd.read_csv('jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "stock_list = dframe.SecuritiesCode.unique()\n",
        "dframe_1 = dframe.drop(['Open', 'High', 'Low', 'Volume', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'], axis=1)\n",
        "dframe_1 = dframe_1[~dframe_1['Close'].isnull()] #Getting rid of null values for this experiment\n",
        "dframe_1 = dframe_1.sort_values(['SecuritiesCode', 'Date'], ascending=[True, True]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WGnujAZM0RNZ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = TSDataset(dframe_1, seq_len=128)\n",
        "dloader = DataLoader(dset, batch_size=128, shuffle=False, num_workers=1)\n",
        "train_batch = next(iter(dloader))\n",
        "train_batch['sequence'].shape, train_batch['date'].shape, train_batch['mask'].shape, train_batch['target'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sypud6WYSVtY",
        "outputId": "8dec4d84-f3c5-417b-b0cc-5a336de567c3"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 128, 1]),\n",
              " torch.Size([128, 128, 3]),\n",
              " torch.Size([128, 128, 1]),\n",
              " torch.Size([128]))"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####Overfitting a single batch\n",
        "\n",
        "#May be a good idea to normalize the data\n",
        "#consider another criterion\n",
        "#consider modifying model params\n",
        "#consider encoding the vector dates instead of the sequence ---------done\n",
        "model = TSTransformer(in_features=1, time_features=7)\n",
        "\n",
        "crit = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "total_loss = []\n",
        "for epoch in range(1000):\n",
        "  out = model(train_batch['sequence'].float(), date_vec=train_batch['date'].float(), padding_mask=train_batch['mask'].squeeze(-1).float())\n",
        "  loss = crit(out.squeeze(-1), train_batch['target'].float())\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  total_loss.append(loss.item())\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0cKwsFUDevl",
        "outputId": "63548d0c-509d-4708-f26f-46badc3ad48b"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2159.4543, grad_fn=<MseLossBackward0>)\n",
            "tensor(1532.0428, grad_fn=<MseLossBackward0>)\n",
            "tensor(2359.6038, grad_fn=<MseLossBackward0>)\n",
            "tensor(2579.9548, grad_fn=<MseLossBackward0>)\n",
            "tensor(1630.9772, grad_fn=<MseLossBackward0>)\n",
            "tensor(629.8348, grad_fn=<MseLossBackward0>)\n",
            "tensor(351.6970, grad_fn=<MseLossBackward0>)\n",
            "tensor(1236.2104, grad_fn=<MseLossBackward0>)\n",
            "tensor(3410.8938, grad_fn=<MseLossBackward0>)\n",
            "tensor(6378.8721, grad_fn=<MseLossBackward0>)\n",
            "tensor(8668.2363, grad_fn=<MseLossBackward0>)\n",
            "tensor(8992.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(7503.7090, grad_fn=<MseLossBackward0>)\n",
            "tensor(5091.9565, grad_fn=<MseLossBackward0>)\n",
            "tensor(2670.5308, grad_fn=<MseLossBackward0>)\n",
            "tensor(984.5673, grad_fn=<MseLossBackward0>)\n",
            "tensor(613.2903, grad_fn=<MseLossBackward0>)\n",
            "tensor(2000.4611, grad_fn=<MseLossBackward0>)\n",
            "tensor(5464.9893, grad_fn=<MseLossBackward0>)\n",
            "tensor(11175.4141, grad_fn=<MseLossBackward0>)\n",
            "tensor(19070.4668, grad_fn=<MseLossBackward0>)\n",
            "tensor(28700.3516, grad_fn=<MseLossBackward0>)\n",
            "tensor(38987.4883, grad_fn=<MseLossBackward0>)\n",
            "tensor(48080.3477, grad_fn=<MseLossBackward0>)\n",
            "tensor(53783.8555, grad_fn=<MseLossBackward0>)\n",
            "tensor(54713.6953, grad_fn=<MseLossBackward0>)\n",
            "tensor(51027.9961, grad_fn=<MseLossBackward0>)\n",
            "tensor(43956.1758, grad_fn=<MseLossBackward0>)\n",
            "tensor(34997.0977, grad_fn=<MseLossBackward0>)\n",
            "tensor(25499.0762, grad_fn=<MseLossBackward0>)\n",
            "tensor(16563.2773, grad_fn=<MseLossBackward0>)\n",
            "tensor(9068.2344, grad_fn=<MseLossBackward0>)\n",
            "tensor(3715.7871, grad_fn=<MseLossBackward0>)\n",
            "tensor(1068.1494, grad_fn=<MseLossBackward0>)\n",
            "tensor(1572.4814, grad_fn=<MseLossBackward0>)\n",
            "tensor(5572.8784, grad_fn=<MseLossBackward0>)\n",
            "tensor(13312.3828, grad_fn=<MseLossBackward0>)\n",
            "tensor(24926.1445, grad_fn=<MseLossBackward0>)\n",
            "tensor(40424.1992, grad_fn=<MseLossBackward0>)\n",
            "tensor(59663.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(82308.8984, grad_fn=<MseLossBackward0>)\n",
            "tensor(107778.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(135180.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(163252.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(190336.7344, grad_fn=<MseLossBackward0>)\n",
            "tensor(214440.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(233444.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(245465.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(249276.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(244585.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(232040.7656, grad_fn=<MseLossBackward0>)\n",
            "tensor(212988.6406, grad_fn=<MseLossBackward0>)\n",
            "tensor(189138.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(162278.8906, grad_fn=<MseLossBackward0>)\n",
            "tensor(134099.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(106103.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(79582.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(55621.1641, grad_fn=<MseLossBackward0>)\n",
            "tensor(35123.6211, grad_fn=<MseLossBackward0>)\n",
            "tensor(18828.8633, grad_fn=<MseLossBackward0>)\n",
            "tensor(7334.8506, grad_fn=<MseLossBackward0>)\n",
            "tensor(1114.8855, grad_fn=<MseLossBackward0>)\n",
            "tensor(531.5959, grad_fn=<MseLossBackward0>)\n",
            "tensor(5846.2549, grad_fn=<MseLossBackward0>)\n",
            "tensor(17225.3633, grad_fn=<MseLossBackward0>)\n",
            "tensor(34744.1719, grad_fn=<MseLossBackward0>)\n",
            "tensor(58386.9648, grad_fn=<MseLossBackward0>)\n",
            "tensor(88045.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(123516.3672, grad_fn=<MseLossBackward0>)\n",
            "tensor(164492.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(210554.1094, grad_fn=<MseLossBackward0>)\n",
            "tensor(261160.7031, grad_fn=<MseLossBackward0>)\n",
            "tensor(315635.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(373153.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(432732.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(493221.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(553299.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(611486.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(666161.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(715615.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(758122.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(792033.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(815900.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(828602.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(829455.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(818292.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(795486.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(761916.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(718882.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(667990.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(611020.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(549818.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(486195.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(421865.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(358400.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(297205.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(239513.5469, grad_fn=<MseLossBackward0>)\n",
            "tensor(186389.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(138739.2344, grad_fn=<MseLossBackward0>)\n",
            "tensor(97317.8672, grad_fn=<MseLossBackward0>)\n",
            "tensor(62746.2422, grad_fn=<MseLossBackward0>)\n",
            "tensor(35521.8047, grad_fn=<MseLossBackward0>)\n",
            "tensor(16030.6172, grad_fn=<MseLossBackward0>)\n",
            "tensor(4558.0396, grad_fn=<MseLossBackward0>)\n",
            "tensor(1298.6838, grad_fn=<MseLossBackward0>)\n",
            "tensor(6363.7397, grad_fn=<MseLossBackward0>)\n",
            "tensor(19788.2383, grad_fn=<MseLossBackward0>)\n",
            "tensor(41536.3047, grad_fn=<MseLossBackward0>)\n",
            "tensor(71505.4297, grad_fn=<MseLossBackward0>)\n",
            "tensor(109529.0234, grad_fn=<MseLossBackward0>)\n",
            "tensor(155380.2031, grad_fn=<MseLossBackward0>)\n",
            "tensor(208772.8906, grad_fn=<MseLossBackward0>)\n",
            "tensor(269361.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(336742.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(410451.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(489963.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(574696.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(664005.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(757179.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(853441.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(951948.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(1051790.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1151997.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1251537., grad_fn=<MseLossBackward0>)\n",
            "tensor(1349319.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1444210., grad_fn=<MseLossBackward0>)\n",
            "tensor(1535034.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1620595.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1699692.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1771137., grad_fn=<MseLossBackward0>)\n",
            "tensor(1833786.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1886570.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1928520., grad_fn=<MseLossBackward0>)\n",
            "tensor(1958806.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1976767.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1981944.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1974098.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1953227.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1919573.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1873627.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1816105.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1747932.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1670209.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1584180.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1491188.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1392640., grad_fn=<MseLossBackward0>)\n",
            "tensor(1289971.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1184611.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1077954.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(971333.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(866002.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(763129.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(663781.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(568928.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(479433., grad_fn=<MseLossBackward0>)\n",
            "tensor(396057.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(319463.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(250220.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(188807.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(135617.1719, grad_fn=<MseLossBackward0>)\n",
            "tensor(90965.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(55095.4609, grad_fn=<MseLossBackward0>)\n",
            "tensor(28183.3867, grad_fn=<MseLossBackward0>)\n",
            "tensor(10342.6475, grad_fn=<MseLossBackward0>)\n",
            "tensor(1630.5374, grad_fn=<MseLossBackward0>)\n",
            "tensor(2051.6213, grad_fn=<MseLossBackward0>)\n",
            "tensor(11562.2686, grad_fn=<MseLossBackward0>)\n",
            "tensor(30073.9883, grad_fn=<MseLossBackward0>)\n",
            "tensor(57456.9766, grad_fn=<MseLossBackward0>)\n",
            "tensor(93541.5703, grad_fn=<MseLossBackward0>)\n",
            "tensor(138123., grad_fn=<MseLossBackward0>)\n",
            "tensor(190962.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(251785.7344, grad_fn=<MseLossBackward0>)\n",
            "tensor(320290.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(396141.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(478975.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(568400.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(663998.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(765324.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(871907.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(983250.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1098829.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1218097.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1340482.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1465390.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1592204.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1720288.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1848986., grad_fn=<MseLossBackward0>)\n",
            "tensor(1977622.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2105499.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2231904.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2356107.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2477371.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2594958., grad_fn=<MseLossBackward0>)\n",
            "tensor(2708136., grad_fn=<MseLossBackward0>)\n",
            "tensor(2816178.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2918369.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3014009.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3102422., grad_fn=<MseLossBackward0>)\n",
            "tensor(3182956.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3255006.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3318008.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3371453.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3414890., grad_fn=<MseLossBackward0>)\n",
            "tensor(3447935., grad_fn=<MseLossBackward0>)\n",
            "tensor(3470281.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3481701.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3482049.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3471263., grad_fn=<MseLossBackward0>)\n",
            "tensor(3449367., grad_fn=<MseLossBackward0>)\n",
            "tensor(3416475.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3372796., grad_fn=<MseLossBackward0>)\n",
            "tensor(3318630.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3254374.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3180514.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3097609.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3006287.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2907243., grad_fn=<MseLossBackward0>)\n",
            "tensor(2801216.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2688994.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2571392.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2449255.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2323435.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2194792., grad_fn=<MseLossBackward0>)\n",
            "tensor(2064186.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1932463.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1800448.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1668951.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1538740.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1410548.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1285072., grad_fn=<MseLossBackward0>)\n",
            "tensor(1162957.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1044801.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(931156.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(822523.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(719358.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(622067.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(531007.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(446496.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(368808.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(298177.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(234800.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(178835.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(130405.4297, grad_fn=<MseLossBackward0>)\n",
            "tensor(89598.7422, grad_fn=<MseLossBackward0>)\n",
            "tensor(56472.2656, grad_fn=<MseLossBackward0>)\n",
            "tensor(31052.2773, grad_fn=<MseLossBackward0>)\n",
            "tensor(13336.4580, grad_fn=<MseLossBackward0>)\n",
            "tensor(3296.3120, grad_fn=<MseLossBackward0>)\n",
            "tensor(878.6290, grad_fn=<MseLossBackward0>)\n",
            "tensor(6007.9312, grad_fn=<MseLossBackward0>)\n",
            "tensor(18587.7539, grad_fn=<MseLossBackward0>)\n",
            "tensor(38501.2773, grad_fn=<MseLossBackward0>)\n",
            "tensor(65614.5781, grad_fn=<MseLossBackward0>)\n",
            "tensor(99777.6484, grad_fn=<MseLossBackward0>)\n",
            "tensor(140824.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(188571.1094, grad_fn=<MseLossBackward0>)\n",
            "tensor(242822.3281, grad_fn=<MseLossBackward0>)\n",
            "tensor(303367.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(369984.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(442439.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(520486.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(603869.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(692325.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(785578.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(883343.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(985328.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1091232.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1200745.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1313552.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1429330.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1547750.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1668478.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1791171.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1915483.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(2041057.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(2167535.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2294550.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2421738.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2548730.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2675161.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2800668.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2924882.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3047437., grad_fn=<MseLossBackward0>)\n",
            "tensor(3167962.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3286086.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3401437.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3513649.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3622362., grad_fn=<MseLossBackward0>)\n",
            "tensor(3727226.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3827905.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3924068., grad_fn=<MseLossBackward0>)\n",
            "tensor(4015395., grad_fn=<MseLossBackward0>)\n",
            "tensor(4101576.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4182324.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(4257366.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4326452., grad_fn=<MseLossBackward0>)\n",
            "tensor(4389349., grad_fn=<MseLossBackward0>)\n",
            "tensor(4445847., grad_fn=<MseLossBackward0>)\n",
            "tensor(4495752., grad_fn=<MseLossBackward0>)\n",
            "tensor(4538880.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4575076.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4604201.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4626146., grad_fn=<MseLossBackward0>)\n",
            "tensor(4640828.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4648190.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4648206., grad_fn=<MseLossBackward0>)\n",
            "tensor(4640871.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4626219., grad_fn=<MseLossBackward0>)\n",
            "tensor(4604307., grad_fn=<MseLossBackward0>)\n",
            "tensor(4575222.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4539081.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4496022., grad_fn=<MseLossBackward0>)\n",
            "tensor(4446205., grad_fn=<MseLossBackward0>)\n",
            "tensor(4389817.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4327066., grad_fn=<MseLossBackward0>)\n",
            "tensor(4258184., grad_fn=<MseLossBackward0>)\n",
            "tensor(4183427., grad_fn=<MseLossBackward0>)\n",
            "tensor(4103068., grad_fn=<MseLossBackward0>)\n",
            "tensor(4017403.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3926742.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3831413.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3731765.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3628155.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3520949.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3410521.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3297247.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3181509.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3063691.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2944186.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2823373.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2701634.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2579344.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2456873.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2334583., grad_fn=<MseLossBackward0>)\n",
            "tensor(2212833., grad_fn=<MseLossBackward0>)\n",
            "tensor(2091967.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1972325.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1854226.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1737978.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1623878.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1512196., grad_fn=<MseLossBackward0>)\n",
            "tensor(1403188.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1297100.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1194156.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1094568.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(998526.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(906214.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(817798.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(733432.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(653258.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(577400.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(505971.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(439068.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(376773.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(319154.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(266265.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(218145.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(174823.2969, grad_fn=<MseLossBackward0>)\n",
            "tensor(136314.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(102626.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(73755.9844, grad_fn=<MseLossBackward0>)\n",
            "tensor(49691.0586, grad_fn=<MseLossBackward0>)\n",
            "tensor(30411.3672, grad_fn=<MseLossBackward0>)\n",
            "tensor(15888.6445, grad_fn=<MseLossBackward0>)\n",
            "tensor(6086.6328, grad_fn=<MseLossBackward0>)\n",
            "tensor(960.6721, grad_fn=<MseLossBackward0>)\n",
            "tensor(458.7422, grad_fn=<MseLossBackward0>)\n",
            "tensor(4521.8262, grad_fn=<MseLossBackward0>)\n",
            "tensor(13083.3926, grad_fn=<MseLossBackward0>)\n",
            "tensor(26071.1602, grad_fn=<MseLossBackward0>)\n",
            "tensor(43406.5117, grad_fn=<MseLossBackward0>)\n",
            "tensor(65006.0977, grad_fn=<MseLossBackward0>)\n",
            "tensor(90782.3516, grad_fn=<MseLossBackward0>)\n",
            "tensor(120643.2734, grad_fn=<MseLossBackward0>)\n",
            "tensor(154491.9844, grad_fn=<MseLossBackward0>)\n",
            "tensor(192227.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(233745.9844, grad_fn=<MseLossBackward0>)\n",
            "tensor(278937.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(327689.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(379886.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(435409.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(494133.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(555933.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(620679.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(688239.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(758481.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(831270.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(906471., grad_fn=<MseLossBackward0>)\n",
            "tensor(983952.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(1063580.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1145218.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1228729.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1313972.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1400807.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1489094., grad_fn=<MseLossBackward0>)\n",
            "tensor(1578688.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1669441.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1761200.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1853815.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1947129.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2040990., grad_fn=<MseLossBackward0>)\n",
            "tensor(2135240., grad_fn=<MseLossBackward0>)\n",
            "tensor(2229730., grad_fn=<MseLossBackward0>)\n",
            "tensor(2324312., grad_fn=<MseLossBackward0>)\n",
            "tensor(2418837.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2513162.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2607141.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2700635.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2793504.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2885609.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2976819.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3067003.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3156032.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3243772., grad_fn=<MseLossBackward0>)\n",
            "tensor(3330089., grad_fn=<MseLossBackward0>)\n",
            "tensor(3414846., grad_fn=<MseLossBackward0>)\n",
            "tensor(3497897.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3579101., grad_fn=<MseLossBackward0>)\n",
            "tensor(3658312., grad_fn=<MseLossBackward0>)\n",
            "tensor(3735385.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3810202.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3882650.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3952617.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4019996.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(4084692.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(4146631.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4205728., grad_fn=<MseLossBackward0>)\n",
            "tensor(4261915.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4315126., grad_fn=<MseLossBackward0>)\n",
            "tensor(4365289.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4412338.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4456207., grad_fn=<MseLossBackward0>)\n",
            "tensor(4496812., grad_fn=<MseLossBackward0>)\n",
            "tensor(4534075.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4567921.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4598279., grad_fn=<MseLossBackward0>)\n",
            "tensor(4625106.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4648370.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4668064., grad_fn=<MseLossBackward0>)\n",
            "tensor(4684178.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4696706.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4705657.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4711059., grad_fn=<MseLossBackward0>)\n",
            "tensor(4712929., grad_fn=<MseLossBackward0>)\n",
            "tensor(4711301., grad_fn=<MseLossBackward0>)\n",
            "tensor(4706191.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4697621.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4685586.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4670100., grad_fn=<MseLossBackward0>)\n",
            "tensor(4651166.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4628825., grad_fn=<MseLossBackward0>)\n",
            "tensor(4603118., grad_fn=<MseLossBackward0>)\n",
            "tensor(4574114., grad_fn=<MseLossBackward0>)\n",
            "tensor(4541877., grad_fn=<MseLossBackward0>)\n",
            "tensor(4506484., grad_fn=<MseLossBackward0>)\n",
            "tensor(4468031., grad_fn=<MseLossBackward0>)\n",
            "tensor(4426601.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4382296., grad_fn=<MseLossBackward0>)\n",
            "tensor(4335220.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4285475.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4233153.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(4178346., grad_fn=<MseLossBackward0>)\n",
            "tensor(4121127.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(4061572.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3999756., grad_fn=<MseLossBackward0>)\n",
            "tensor(3935774.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3869731.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3801739.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3731926.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3660446.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3587420.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3512963., grad_fn=<MseLossBackward0>)\n",
            "tensor(3437209., grad_fn=<MseLossBackward0>)\n",
            "tensor(3360278.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3282306.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3203405., grad_fn=<MseLossBackward0>)\n",
            "tensor(3123688.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3043271.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2962249.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2880723.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2798802., grad_fn=<MseLossBackward0>)\n",
            "tensor(2716584., grad_fn=<MseLossBackward0>)\n",
            "tensor(2634181.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2551713., grad_fn=<MseLossBackward0>)\n",
            "tensor(2469298., grad_fn=<MseLossBackward0>)\n",
            "tensor(2387063.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2305116.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2223566.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2142515.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2062064.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1982303.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1903323.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1825199.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1748016.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1671848.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1596767., grad_fn=<MseLossBackward0>)\n",
            "tensor(1522840.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1450133.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1378716.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1308664.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1240047.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1172928.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1107375.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1043438.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(981166.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(920603.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(861792.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(804774.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(749578.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(696229.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(644754.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(595174.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(547510.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(501782.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(458008.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(416209.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(376400.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(338589.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(302798.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(269034.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(237301.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(207602.9531, grad_fn=<MseLossBackward0>)\n",
            "tensor(179936.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(154298.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(130681.2344, grad_fn=<MseLossBackward0>)\n",
            "tensor(109075.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(89470.5391, grad_fn=<MseLossBackward0>)\n",
            "tensor(71855.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(56215.1055, grad_fn=<MseLossBackward0>)\n",
            "tensor(42535.8320, grad_fn=<MseLossBackward0>)\n",
            "tensor(30801.9336, grad_fn=<MseLossBackward0>)\n",
            "tensor(20997.4707, grad_fn=<MseLossBackward0>)\n",
            "tensor(13104.6777, grad_fn=<MseLossBackward0>)\n",
            "tensor(7104.4004, grad_fn=<MseLossBackward0>)\n",
            "tensor(2976.0911, grad_fn=<MseLossBackward0>)\n",
            "tensor(697.4648, grad_fn=<MseLossBackward0>)\n",
            "tensor(244.8915, grad_fn=<MseLossBackward0>)\n",
            "tensor(1593.0006, grad_fn=<MseLossBackward0>)\n",
            "tensor(4715.0259, grad_fn=<MseLossBackward0>)\n",
            "tensor(9582.7334, grad_fn=<MseLossBackward0>)\n",
            "tensor(16166.7100, grad_fn=<MseLossBackward0>)\n",
            "tensor(24436.5098, grad_fn=<MseLossBackward0>)\n",
            "tensor(34361.2344, grad_fn=<MseLossBackward0>)\n",
            "tensor(45909.4844, grad_fn=<MseLossBackward0>)\n",
            "tensor(59049.0078, grad_fn=<MseLossBackward0>)\n",
            "tensor(73748.1094, grad_fn=<MseLossBackward0>)\n",
            "tensor(89973.6172, grad_fn=<MseLossBackward0>)\n",
            "tensor(107690.6172, grad_fn=<MseLossBackward0>)\n",
            "tensor(126863.6484, grad_fn=<MseLossBackward0>)\n",
            "tensor(147455.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(169428.8594, grad_fn=<MseLossBackward0>)\n",
            "tensor(192744.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(217361.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(243239.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(270335.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(298605.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(328004.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(358492.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(390025.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(422556.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(456042.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(490443.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(525716.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(561825.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(598727.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(636383.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(674755.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(713804.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(753490.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(793767.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(834594.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(875932.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(917739.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(959981.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1002619.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(1045613.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(1088921.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1132512., grad_fn=<MseLossBackward0>)\n",
            "tensor(1176345.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1220376.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1264563.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1308878.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1353293.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1397774.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1442284.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1486788.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1531248.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1575628.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1619895., grad_fn=<MseLossBackward0>)\n",
            "tensor(1664008.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1707931.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1751627.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1795063., grad_fn=<MseLossBackward0>)\n",
            "tensor(1838207., grad_fn=<MseLossBackward0>)\n",
            "tensor(1881023.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1923486.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1965564.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2007226.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(2048441.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(2089192., grad_fn=<MseLossBackward0>)\n",
            "tensor(2129452.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2169193., grad_fn=<MseLossBackward0>)\n",
            "tensor(2208370., grad_fn=<MseLossBackward0>)\n",
            "tensor(2246961., grad_fn=<MseLossBackward0>)\n",
            "tensor(2284947.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2322298.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2358997., grad_fn=<MseLossBackward0>)\n",
            "tensor(2395024.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2430340.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2464921., grad_fn=<MseLossBackward0>)\n",
            "tensor(2498751.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2531817.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2564086., grad_fn=<MseLossBackward0>)\n",
            "tensor(2595541.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2626143.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2655870.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2684719., grad_fn=<MseLossBackward0>)\n",
            "tensor(2712649.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2739659.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2765736., grad_fn=<MseLossBackward0>)\n",
            "tensor(2790862., grad_fn=<MseLossBackward0>)\n",
            "tensor(2815037.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2838236.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2860466., grad_fn=<MseLossBackward0>)\n",
            "tensor(2881705.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2901935.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2921138., grad_fn=<MseLossBackward0>)\n",
            "tensor(2939280.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2956412.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2972506., grad_fn=<MseLossBackward0>)\n",
            "tensor(2987561.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3001592., grad_fn=<MseLossBackward0>)\n",
            "tensor(3014580.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3026532., grad_fn=<MseLossBackward0>)\n",
            "tensor(3037431.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3047249., grad_fn=<MseLossBackward0>)\n",
            "tensor(3055973., grad_fn=<MseLossBackward0>)\n",
            "tensor(3063585.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3070104.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3075529., grad_fn=<MseLossBackward0>)\n",
            "tensor(3079863., grad_fn=<MseLossBackward0>)\n",
            "tensor(3083109., grad_fn=<MseLossBackward0>)\n",
            "tensor(3085288.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3086396.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3086417.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3085376.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3083287., grad_fn=<MseLossBackward0>)\n",
            "tensor(3080181., grad_fn=<MseLossBackward0>)\n",
            "tensor(3076102.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3071056.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3065043., grad_fn=<MseLossBackward0>)\n",
            "tensor(3058069.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(3050128.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3041193.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3031266.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3020365.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(3008514., grad_fn=<MseLossBackward0>)\n",
            "tensor(2995713., grad_fn=<MseLossBackward0>)\n",
            "tensor(2981960., grad_fn=<MseLossBackward0>)\n",
            "tensor(2967289.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2951731.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2935279.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2918015.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2899974.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2881206.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2861688.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2841457.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2820514.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2798839., grad_fn=<MseLossBackward0>)\n",
            "tensor(2776467.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2753392.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2729637.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2705212.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2680122.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2654377., grad_fn=<MseLossBackward0>)\n",
            "tensor(2628010.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2601085.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2573637.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2545687., grad_fn=<MseLossBackward0>)\n",
            "tensor(2517273.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2488411.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2459108., grad_fn=<MseLossBackward0>)\n",
            "tensor(2429367.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2399208.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2368638.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2337690.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2306402.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2274789.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(2242878.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2210703., grad_fn=<MseLossBackward0>)\n",
            "tensor(2178245.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2145552.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(2112678., grad_fn=<MseLossBackward0>)\n",
            "tensor(2079629.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(2046412.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(2013039.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1979486.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1945764.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1911921.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1878003.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1844050.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1810107.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1776200.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1742356.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1708610., grad_fn=<MseLossBackward0>)\n",
            "tensor(1674963.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1641433., grad_fn=<MseLossBackward0>)\n",
            "tensor(1608017.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1574685., grad_fn=<MseLossBackward0>)\n",
            "tensor(1541445.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1508299.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1475264.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1442369.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1409632.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1377106.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1344837.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1312864.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1281220.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1249927.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1218989.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(1188423.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(1158204.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1128306.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1098741.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1069509.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(1040587.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(1012030.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(983851.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(956107.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(928819.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(902020.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(875719.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(849920.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(824650.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(799893.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(775644., grad_fn=<MseLossBackward0>)\n",
            "tensor(751888.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(728598.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(705789.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(683460.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(661610.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(640254.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(619417.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(599111.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(579347.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(560125.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(541475.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(523404.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(505906.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(488964.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(472568.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(456698.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(441345.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(426512.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(412184.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(398363.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(385052.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(372263.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(360000.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(348250.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(337022.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(326320.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(316141.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(306474.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(297325.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(288686., grad_fn=<MseLossBackward0>)\n",
            "tensor(280535.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(272856.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(265644.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(258894.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(252598.3281, grad_fn=<MseLossBackward0>)\n",
            "tensor(246753.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(241359.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(236402.2656, grad_fn=<MseLossBackward0>)\n",
            "tensor(231883.5469, grad_fn=<MseLossBackward0>)\n",
            "tensor(227794.4531, grad_fn=<MseLossBackward0>)\n",
            "tensor(224129.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(220875.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(218028.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(215566.0781, grad_fn=<MseLossBackward0>)\n",
            "tensor(213491.6094, grad_fn=<MseLossBackward0>)\n",
            "tensor(211790.3281, grad_fn=<MseLossBackward0>)\n",
            "tensor(210441.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(209424.0781, grad_fn=<MseLossBackward0>)\n",
            "tensor(208720.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(208315.8281, grad_fn=<MseLossBackward0>)\n",
            "tensor(208194.3906, grad_fn=<MseLossBackward0>)\n",
            "tensor(208357.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(208795.8281, grad_fn=<MseLossBackward0>)\n",
            "tensor(209505.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(210483.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(211719.2031, grad_fn=<MseLossBackward0>)\n",
            "tensor(213207.5156, grad_fn=<MseLossBackward0>)\n",
            "tensor(214937.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(216900.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(219089.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(221490.2031, grad_fn=<MseLossBackward0>)\n",
            "tensor(224080.0938, grad_fn=<MseLossBackward0>)\n",
            "tensor(226846.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(229770.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(232837.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(236040.0156, grad_fn=<MseLossBackward0>)\n",
            "tensor(239365.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(242801.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(246344.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(249987.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(253741.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(257593.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(261539.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(265573., grad_fn=<MseLossBackward0>)\n",
            "tensor(269678.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(273852.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(278082.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(282338.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(286612.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(290895.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(295175.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(299449.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(303707.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(307941.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(312149.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(316328.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(320481.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(324616.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(328726., grad_fn=<MseLossBackward0>)\n",
            "tensor(332812.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(336862.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(340876.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(344835.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(348748.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(352606.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(356404.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(360131.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(363780.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(367363.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(370864.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(374282.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(377622.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(380888.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(384074.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(387181.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(390211.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(393181.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(396091.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(398939.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(401736.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(404477.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(407152.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(409740.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(412252.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(414692.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(417066.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(419350.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(421550.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(423667.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(425714.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(427684.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(429587.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(431421.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(433173.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(434862.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(436481.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(438036.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(439541.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(440994.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(442383.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(443718.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(445003.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(446224.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(447392.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(448504.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(449564.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(450581.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(451553.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(452492., grad_fn=<MseLossBackward0>)\n",
            "tensor(453396.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(454267.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(455103.0938, grad_fn=<MseLossBackward0>)\n",
            "tensor(455904.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(456671.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(457423., grad_fn=<MseLossBackward0>)\n",
            "tensor(458157.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(458865.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(459548.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(460207.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(460829.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(461427.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(461991.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(462521.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(463032.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(463528.7812, grad_fn=<MseLossBackward0>)\n",
            "tensor(464010.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(464484.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(464945.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(465408.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(465870.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(466314.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(466753.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(467182.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(467605.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(468018.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(468412.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(468790.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(469141.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(469486.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(469817.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(470142.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(470465.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(470784.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(471090.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(471384.0938, grad_fn=<MseLossBackward0>)\n",
            "tensor(471658.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(471904.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(472141.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(472367.0938, grad_fn=<MseLossBackward0>)\n",
            "tensor(472565.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(472743.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(472901.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(473030.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(473111.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(473151.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(473132.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(473080.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(472995.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(472871.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(472700.8438, grad_fn=<MseLossBackward0>)\n",
            "tensor(472492.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(472221.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(471890.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(471513.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(471093.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(470625.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(470103.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(469527.2500, grad_fn=<MseLossBackward0>)\n",
            "tensor(468882.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(468173.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(467403.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(466562.2812, grad_fn=<MseLossBackward0>)\n",
            "tensor(465644.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(464667.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(463633.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(462541.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(461394.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(460181.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(458908.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(457587.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(456214.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(454791.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(453324.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(451785.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(450188.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(448540.9375, grad_fn=<MseLossBackward0>)\n",
            "tensor(446836.6562, grad_fn=<MseLossBackward0>)\n",
            "tensor(445074.2188, grad_fn=<MseLossBackward0>)\n",
            "tensor(443253.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(441386.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(439474.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(437519.9688, grad_fn=<MseLossBackward0>)\n",
            "tensor(435514.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(433461.5312, grad_fn=<MseLossBackward0>)\n",
            "tensor(431365., grad_fn=<MseLossBackward0>)\n",
            "tensor(429225.6250, grad_fn=<MseLossBackward0>)\n",
            "tensor(427031.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(424786.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(422502.6875, grad_fn=<MseLossBackward0>)\n",
            "tensor(420178.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(417816.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(415420.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(412999., grad_fn=<MseLossBackward0>)\n",
            "tensor(410553.5000, grad_fn=<MseLossBackward0>)\n",
            "tensor(408077.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(405567.1562, grad_fn=<MseLossBackward0>)\n",
            "tensor(403027.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(400465., grad_fn=<MseLossBackward0>)\n",
            "tensor(397876.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(395263.7188, grad_fn=<MseLossBackward0>)\n",
            "tensor(392626.4375, grad_fn=<MseLossBackward0>)\n",
            "tensor(389957.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(387258.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(384541.3750, grad_fn=<MseLossBackward0>)\n",
            "tensor(381803.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(379038.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(376250.3125, grad_fn=<MseLossBackward0>)\n",
            "tensor(373436.9062, grad_fn=<MseLossBackward0>)\n",
            "tensor(370595.1875, grad_fn=<MseLossBackward0>)\n",
            "tensor(367727.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(364846.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(361950.3438, grad_fn=<MseLossBackward0>)\n",
            "tensor(359043.8750, grad_fn=<MseLossBackward0>)\n",
            "tensor(356125.7500, grad_fn=<MseLossBackward0>)\n",
            "tensor(353196.8125, grad_fn=<MseLossBackward0>)\n",
            "tensor(350260.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(347320.5625, grad_fn=<MseLossBackward0>)\n",
            "tensor(344381.5938, grad_fn=<MseLossBackward0>)\n",
            "tensor(341436.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(338493.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(335544.4688, grad_fn=<MseLossBackward0>)\n",
            "tensor(332591.4062, grad_fn=<MseLossBackward0>)\n",
            "tensor(329636.4375, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_loss = torch.tensor(total_loss)\n",
        "plt.plot(tensor_loss.numpy())"
      ],
      "metadata": {
        "id": "IfxiWlv1cyVJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "dcadc8bc-29eb-48ef-8ca7-9c67491485b4"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f81321755d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 216
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eXib53nme7/Y94UEd4miZC2WYluWLcuOdztO6kncpOdM0hOnSTqtO87MZKZpT9rp5MzVK6eZJdNpT9dpJ802TdpmmSzN4iR2He92HNmyLcnaSWvjDpIAiI3Y3/PHhxeEKBD4AHwbgOd3XbokAiDwgqBuPLifjXHOQRAEQRgXk94HIAiCIOpDQk0QBGFwSKgJgiAMDgk1QRCEwSGhJgiCMDgk1ARBEAZHNaFmjH2ZMRZmjJ2QeftfZoydYoydZIx9Ta1zEQRBdBpMrTpqxtjdAJIAvso5v67BbXcB+N8A7uecRxljg5zzsCoHIwiC6DBUi6g5588DiFRfxhi7hjH2OGPsNcbYC4yxa8tX/UsAf8U5j5a/l0SaIAiijNYe9ecB/DvO+c0AfgfAX5cv3w1gN2PsJcbYzxljD2p8LoIgCMNi0eqBGGMeALcD+BZjTFxsrzrHLgD3AtgC4HnG2PWc85hW5yMIgjAqmgk1pOg9xjm/scZ1MwAOc87zAC4wxs5BEu5XNTwfQRCEIdHM+uCcxyGJ8AcAgEnsL1/9PUjRNBhjIUhWyHmtzkYQBGFk1CzP+zqAlwHsYYzNMMYeAfArAB5hjB0DcBLA+8o3fwLACmPsFIBnAPwu53xFrbMRBEF0EqqV5xEEQRDKQJ2JBEEQBkeVZGIoFOITExNq3DVBEERX8tprry1zzgdqXaeKUE9MTODIkSNq3DVBEERXwhi7tNl1ZH0QBEEYHBJqgiAIg0NCTRAEYXBIqAmCIAwOCTVBEITBIaEmCIIwOCTUBEEQBoeEmjA8nHM8dnwOX3/lMvLFkt7HIQjN0XLMKUG0xN///BJ+//snAQBn5uP4g/fV3exGEF0HRdSEoUllC/jvj5/FXbtC+Mht2/CVly/hraWk3sciCE0hoe5hVtN5/Ku/ew3/7SdnYNQpio8dn0MiW8An3rELv/mOXbCaGf7h55f1PhZBaAoJdQ/zueffwuMnF/C5597CS1PGHP/9xMlFjPe5cPO2IAa8dtyzexBPnFww7BsLQagBCXUP8/iJBdx+TT9cNjMePzmv93GuIpMv4mdvLeP+awch9mw+sHcQs7E1nFlI6Hw6gtAOEuoe5fJKGheWU3jXviHcfk0Iz55d0vtIV3F8ZhWZfAl37gxVLrtzl/TvVy9G9DoWQWgOCXWP8vrlKADg1h39ePs1/ZiJrmEpkdX5VFdydFo6443jgcplYwEnhnx2vHYpqtexCEJzSKh7lNMLcdjMJuwc9GDviFe6bD6u86mu5Oh0DFuCToQ89spljDHcvC1YeaMhiF6AhLpHOT2fwM5BD6xmE/aN+MqXGUyoL8dw49bAVZffNB7EdGQN4XhGh1MRhPaQUPcok4sJXDssRdIBlw1DPjvOLhonQbeSzGJuNYP9W64W6uvH/ACAUwZ7YyEItSCh7kGyhSIW4hmM97sql23rd+PySlrHU13JZFhqatldfjOp5tph6RPAWar8IHoEEuoeZC6WAefA1mCVUPe5cCliPKHeNei56jq/y4phn4OEmugZSKh7kOmyIG/tq46oXVhKZJHOFfQ61hVMLibgsVsw4nfUvH7PsJdqqYmegYS6B5mOCqF2Vi4b73cDAC4bJKqeXExi56Cn0uiykT3DXkwtJVGgaXpED0BC3YNMR9ZgM5sw5F2PVscCkmjPrxqjkmIynKxpewh2DXqQK5QwE13T8FQEoQ8k1D3IdDSNsaATJtN6tDpcthgWDCDUyWwBy8ksdgxsLtTbQ9IngAsrKa2ORRC6QULdg8xG17Al6LziskGv1FRiBKEWHvp4lYe+kW1lq+bSMgk10f2QUPcg4XgGg94rk3RWswkhjw2LBmgiuSxDqEMeG9w2My4aqKSQINSChLrH4JxjKZnFoM9+1XVDPgcWDCDUciJqxhgmQm5cJOuD6AFIqHuMaDqPfJFjwHO1UA/7HIawPi5H0vA5LPC7rHVvN9HvxkWyPogegIS6xwgnJCGuGVH7HYawPi6tpK/omtyMiZALM9E1WnhLdD0k1D2GGGW60aMGpIg6ms4jky9qfawrmI6k69oegm39bhRKHLNUokd0OSTUPUY4LoS6tvUBQNe51MUSx0x07Yquyc0QJXrkUxPdDgl1jxEui/BADaEOeW0AgOWkfkK9GM8gVyzJiqhFieFsjCJqorshoe4xwokM3DYz3HbLVdf1uSXxXknmtD5Whbmy6I4GnA1uKdk3FhMj64Poekioe4xwIotBX+1BR/1uKaKOpPQTatHCvtkwpmrMJoZhv4MiaqLrIaHuMSLJXEWQN9LvKVsfKX2tDwAY8TWOqAFpRglF1ES3I1uoGWNmxtgbjLHH1DwQoS7RdA7BTYTaZbPAZTPran3Mr2bgtJrhc15tzdRiLOis2CUE0a00E1F/AsBptQ5CaEMklUOfq7ZQA0Cf26ar9bEQz2DY79h0vOlGxgJOLMQzVEtNdDWyhJoxtgXAewB8Ud3jEGrCOa8bUQNAv8eua9XHwmqmUiYoh7GAEyVujGFSBKEWciPqPwPw7wFsGrYwxh5ljB1hjB1ZWlpS5HCEsqRyReSLHH3uzVuzQ26brtbHwmpGViJRMFYu0SP7g+hmGgo1Y+whAGHO+Wv1bsc5/zzn/CDn/ODAwIBiBySUI1q2NIJ1rI9+jw0rOiUTSyWOxXgGQ00ItSjjo8oPopuRE1HfAeC9jLGLAL4B4H7G2N+reipCFYT33FfH+uhz2xFJ5cA51+pYFZZTWRRKvLmIWgg1VX4QXUxDoeacf4pzvoVzPgHggwCe5px/WPWTEYoTSZcj6noetduGfJEjkdV+ya3wmZvxqB1WM4IuK+YNMEyKINSC6qh7CDnWR6A8WnQ1ndfkTNVUhLqJiBqQ5miHSaiJLqYpoeacP8s5f0itwxDqUrE+6gq1dF00rX1CUSwtaFaoh/3GWHhAEGpBEXUPEU3nYDYxeB2bN5OIiDqmU0RtMTGE3FcPjKqHtPBAv5JCglAbEuoeIprOI+iyXrF9fCNBIdRr+gj1kM9R93y1GPQ5sJLKUtML0bWQUPcQ0VSurj8NAH6ndP2qDtbHYiJTc/NMI4Z9DnCu7xxtglATEuoeIpKq35UIAH6nFFFHdbA+lhO5mrscGzHsl76HfGqiWyGh7iGi6fpzPgDAZjHBY7fo4lEvJbM1Fxo0QqwVo8oPolshoe4hIqk8gnXaxwV+pxWxNW2tj3yxhGg6h1BLEbUk1DTvg+hWSKh7iHgmD5+jsVAHXFbNI2qpG7L2irBG9LlssJoZFuLkURPdCQl1j5AtFJErlOBzNhbqoMuGmMbJRJEIbCWiNpkYBr3U9EJ0LyTUPUIiI7WE16uhFvhdVs3L85aSmy/dlcOQz07JRKJrIaHuEeJl4ZUj1AGn9tbHstiO3kJEDVB3ItHdkFD3CCKiluNRC+ujVNJugp6IqEPe+lUpmyHN+yCPmuhOSKh7hHXrQ14yscSBZE67CXrLiRzcNjNcNnm7Ejcy5HMgmS0gqcPUP4JQGxLqHiGekawMOUtjRdNLLKWd/dFqDbVgsPy9y9SdSHQhJNQ9QiIjPGp51gcATWuplxPtCbX43iUd9z0ShFqQUPcIzVR96DFBbymZbak0T1ARaoqoiS6EhNoATEfSeN9fvYSv/Oyiao8RX8uDMcAjwwMWtdZC3LVguU3rQ1SLkFAT3QgJtQH48ksXcGw6hs88dkq1ZFg8U4DHbpE1QlRUhghfW21yhRJi6XxbEXXQZYPZxEioia6EhNoAvDS1DLOJoVjiOHIxospjJDIFWaV5wHrCMa5R04vYet5ORG0yMfS7bSTURFdCQq0zK8kszi0m8ejdOwAAp+bjqjxOPJOX5U8DgNNqhsXENIuo22kfr2bAa6dkItGVkFDrzFQ4CQB4+45+bAk6cXJOHaFOyBzIBACMMficVsTXtPGoV8q7HPs9rTW7CAa8doqoia6EhFpnLiynAADbQ27sHfHh7EJClceJrxVkR9QA4HNYNIuoI8nGS3flMOAhoSa6ExJqnbmwnILNYsJowInxPhdmomlwrnzrdiKblzU5TyBF1BoJtdiOrkBEvZzMatr6ThBaQEKtM+eXU9jW54LZxLAl6EQmX6pYAUqSyDQbUVsR16g8byWVg9XM4LW31j4uGPDaUShxXRbzEoSakFDrzMJqBmNBJwBgLCD9PRtdU/QxOOfNC7XTollEHU3l0Oe2gbHmto9vRFSNLFNCkegySKh1ZjGewVB559+WoAsAMBtTVqjTuSKKJS47mQiIiFqr8rwc+tztVXwA1PRCdC8k1DpSKJawnMxiyCcJjIio5xQW6mYm5wm0rPqIpLLob7AdXQ7URk50KyTUOrKSyqHEgUGfFFH7nBbYzCbFa4GbmZwn8DksWMtL67vUJpLKIUhCTRCbQkKtI4vljSRDZaFmjKHfY8NKUtlkYjOT8wTr8z7Utz9WUjlFImqP3QKHVfk3OoLQGxJqHVksbyQR1gcgdecpnQyLNzE5T7A+70Nd+yNXKCGRKaBPAaFmjFHTC9GVkFDryMaIGoAqEbWo3mgqmajRvA+x7VwJoQakNzoSaqLbIKHWkXA8AxPDFR/7Qx47VhSOqNf3JbYSUasr1JX2cYWEmroTiW6EhFpHFuNZ9HvssJjXX4Z+jw3LyZyi3YmtVn0AUL3yo9KVqJRQd8lgJs45Ts3Fcfj8CjL5ot7HIXSmvVYwoi0i6auTaAMeO3LFEhJZ+WNJGxHP5GE1Mzis8t+XtY6olRTqSCqHfLEEq7kz45BwIoPf/uZRvDS1AkD6lPVHH7gB9+0Z1PlkhF40/E1mjDkYY68wxo4xxk4yxv5Ai4P1ArF0rrKfUCAmyCm5pDWRycPrsDbV+aeVRx0pR79KCjUAxX1+rUhk8vjIF1/B65di+P2H9uELHz2IkMeG3/jKETx9ZlHv4xE6ISfkyAK4n3O+H8CNAB5kjN2m7rF6g2g6j6D7yqhZzGRWct5Hs5PzAO1mUkfS0oqwQJuT8wSd3p34X350GpPhBL7w0YN45M7teOe+IXz7X9+OvSNefOIbR7GwmtH7iIQONBRqLpEsf2kt/6HxZAoQS+euEigh1EpH1M3aKFrNpI6kspU1WkoQ6uB5H29cjuIbr07jX961A3fuClUu99gt+B8P34R8sYRP/+CEjick9EKWiccYMzPGjgIIA3iSc364xm0eZYwdYYwdWVpaUvqcXQfnHLF0HoENo0eFFRJVcAN4swOZBFrMpI6UBzIpRSWi7kCh/pMnz6HPbcO/e8euq66bCLnx8Xt34omTi3j9clSH0xF6IkuoOedFzvmNALYAOMQYu67GbT7POT/IOT84MDCg9Dm7jkS2gEKJX+VRB1yScK8q6A03s4arGi1mUq8klRXqUIdaH6fm4nhhchkfu3sHPJuMe/31O7ej323DXz41qfHpCL1pKi3OOY8BeAbAg+ocp3eIpSQBFMIscFjNsFlMiK0p51E3s9i2Gi1mUkdSubY3u1TjtJnhsVs6zvr4+8OX4LCa8MFbxje9jdtuwUfevg3PnF2qbAYiegM5VR8DjLFA+d9OAO8EcEbtg3U7Qog3RtQAEFA4kpWsjxaEWoOZ1JFUru3NLhsJlWvRO4VEJo/vvTGLX7xhFH5X/dfpQ7eOw2pm+MrPLmpzOMIQyImoRwA8wxg7DuBVSB71Y+oeq/sRHvTGqg8A8DutiCnkURdLHMlsqx61ujOpSyWOaI1a8naR5n10TnXE4ycWkM4V8fCtm0fTgkGvA++5fgTfeX2GGmF6CDlVH8c55wc45zdwzq/jnH9Gi4N1O2LGRa2ytIBLOaFOivbxJvYlCtSu+lhdy6PElauhFkiDrTonov7Rm/PYEnTiwNaArNv/85u3IJEp4NmzYZVPRhiFzmzd6gKi5TrpjVUfgBRRK5VMjFdGnLZW9aHmTGqluxIFakwgVItYOocXJ5fxnutHZDck3X5NCANeO777+qzKpyOMAgm1Tgjrw19TqG2KC3VLyUSnum3kkcpApvbXcFUz4LUjls5rsvSgXf7p1CIKJY53Xz8i+3vMJob37R/FM2fDlU9mRHdDQq0TsXQOPoflioFMAsmjVuY/YCuT8wQiCk+oVPkRSUlRby2fvh3WuzuNH1U/ezaMIZ8dN2zxN/V9v3RgDPkix+MnFlQ6GWEkSKh1IraW33T9VMBlRSpXRL7YfkTYyuQ8QWUwk0qVHysqRtQAsJwwdrRZKJbw4uQy7t410PQG9reN+rAl6MRPT9P8j16AhFonoun8pvMthB2ihP1RWRrQxL5EgdrWh/DplY+opZ/rUtLYlR/HZlYRzxRwz57mG8QYY3hg7xBemFzGWo6qP7odEmqdiKVzNROJwHoTjBKVH63sSxSobX2spHLw2i2wW8yK3u/6vBRjR9TPnVuCiQF37gw1vnEN3rlvCNlCCS9M0siGboeEWidi6fxVXYkCn4IRdaKFfYmVc6hsfSi1fXwjlW3kBq/8eP7cEvZvDbQ8OfDQ9j54HRayP3oAEmqdqDfRzu9UTiDjmTycVnNLQ/S1qPpQujQPkNrwvXaLoed9JDJ5HJ+J4a4Wo2kAsJpNuHfPIJ4+s6ToRiDCeJBQ6wDnvO5EO1GhoYRAtjo5DwDcNjNMTD3rI5pWR6gBadypkWupX7sURYkDt+7ob+t+7t4VwnIyizMLCYVORhgREmodyORLKJT4pr6xuFwJgWx1ch4gJay8DvUm6EVT+ZqzTpTA6EtuX70YgdnEcGBcXjfiZoi51S9OLitxLMKgkFDrQKJBt6AYc6mEUCcyhZbaxwU+p0W1CXpSRK1sxYcg5LUZOqJ+9UIU14364LK1t7Z0xO/ENQNuvDhFQt3NkFDrQLxBgs9lM8NsYhVBb/exWqn4EPhUiqgz+SLSuaIqyUTA2PM+soUijs7EcMtEnyL3d9euARy+sIJsgcr0uhUSah1INGjrZozBY7cgmVUgol5r3foApDcTNTzqaLnzUslZ1NUMeOxYXcsbUrzenFlFrlDCLduVEeo7doaQyZfw+qWYIvdHGA8Sah2QUzKnlEDGW1waIFBr1Gmk0uyiXjIRMOY28lcvSqu0lIqob9vRB7OJ4cUpqqfuVkiodUBOW7fXYVXE+pDKAFuPqNVaxxUtb7hRK5lYaXoxoE99dDqKiX6XYhUvXocVN24N4KWpFUXujzAeJNQ60CiZKK5rN4mXLRSRLZTaSyY6rKpYHxFhfaiUTKw0vRiw8uP4zCr2y5w9LZdbt/fhxOwq0jl1V6cR+kBCrQNyrA+fAtZHO12JAq/DgkS2gGJJ2YaKypwP1SJq6X6NFlGHExnMr2ZwwxZlhfqW7X0olDiOXiafuhshodaBRCYPxgB3ndIsJawPJYRaRONJhaPqSCoHxmrP41aCdevDWB718elVAMD+JseaNuLmbUGYGHD4QkTR+yWMAQm1DsQzBXjsFphMm4+29Njbj6grk/PaSiYq1yVZTSydg99prTmPWwkcVjO8DuO1kR+bicFsYnjbqLJC7XNYsW/Uh1dIqLsSEmodSMioxPA6pPK8dmY4tDOLev0c6sz7iKTzqpXmCQY8dsMNZjo2s4rdQ144bcpODASkKpLXL0c7YrMN0Rwk1DqQkNHW7XVYUSxxrLWxaVpO0rIRYo610ktuoypNzqsm5LFj2UARNeccx2diitseglu39yFbKOHN2VVV7p/QDxJqHZAzKEmJWdCVfYltVn1U35dSRFI51RKJggGvsSLq6cgaYum84olEgajLJvuj+yCh1oFENt/QjlgX6tYFUolkokj2KV2ip+acD0HIYzNURH18VqrIaHY/olz6PXbsHPTglQtUT91tkFDrQHytcUS9Hsm2E1EXwBjgaWPwjzinkk0vnHNNIuqQx454pmCYNvJTc3FYzQy7h7yqPcYtE0FphKrC5ZSEvpBQ64A8j1oB62Mt37C6pBFikp+S1sdaXmrEUdujriy5NUiJ3qn5OK4Z8MBmUe+/3YHxIOKZAs4vJ1V7DEJ7SKg1Zn1pQCPrQ1gO7Vkf7ZTmAYDFbILHblE0mSjmfKhd9bG+O9EY9sfp+Tj2jfpUfYybxoMAQAOaugwSao1ZXxqgTTKxHX+6+ixKzB0RVOZ8qF314TXOvI+VZBaL8Sz2jagr1DtCbvidVrx+Oarq4xDaQkKtMXK3gnsUSSZuvpexGZSeoBdVec6HwEjzPk7PS6uy1BZqU3lrDAl1d0FCrTEiOdhoop3HZgFrc19hO/sSq/E5lbU+hFCrnUzsdxtn3sepeam2ea/KQg1I9sdkOKnaUmJCe0ioNUZuE4rJxOCxtddGHs/k26qhFvgcViSyyv2nr3jUKlsfoo3cCMnEU3NxjPgdqts9AHBgPADOQQOauggSao1ppq273eUBSkXUXofCEXUqBxNrbwaJXAa8xlhye3o+oUk0DQA3bg2AMeANEuqugYRaY5ppQmlngt56dYkS1oeyHnUkLdVQt1M2KJeQAeZ9ZPJFTC0lVfenBV6HFbsHveRTdxEk1BojN5ko3ab1iDqdK6JY4oolExOZ9gZEVRNN5RFwqR9NA9JgJr3L86bCSRRLXPXSvGpu2hbAG5ep8aVbaCjUjLGtjLFnGGOnGGMnGWOf0OJg3UpzEbWlZW9Yicl51ecoljjSOWU6/CKpnOr+tMAI8z5OzcUBaJNIFFDjS3chJ6IuAPgk53wfgNsAfJwxtk/dY3UvYmmAnLZuj8Pa8sB+JSbnCURCUin7I5pWv31cEPLYkMgUkGljCmG7nFlIwGE1YVufS7PHpMaX7qKhUHPO5znnr5f/nQBwGsCY2gfrVuKZAjw2eW3d7VgfSkzOE1TmjiiUUNQyojbCktvJcAK7Br2aePKCHSE3fA4L+dRdQlMeNWNsAsABAIdrXPcoY+wIY+zI0lJ3rq1P5wr45c+9jN/79vGW76OZBJ/YV9gKcQUm5wnETGoluhM554il85qUqQHGmPdxbjGBXUMeTR9TanwJklB3CbKFmjHmAfAdAL/FOY9vvJ5z/nnO+UHO+cGBgQElz2gYfvzmAl65GME3j0zjraXWvD9pIJO8KNdrtyBXKLU0/S0hs7FG1jkUnEmdyhWRK5ZUn/Mh0Hvex+paHovxrKoT8zZDNL4o2f5P6IMsoWaMWSGJ9D9wzr+r7pGMy0tTy5V/H7nY2nD25iLq1mdBK7EvUVDZm6iA9VHZPq5xRK1XQnFyUWod361xRA2sN74cm6aNL52OnKoPBuBLAE5zzv9E/SMZl1Nzcdy3ZwAum7kyu6FZpKUB8oRajBhtJaGoZNWHz9n+JD/BeleiNuV5/Z5yG7lOEfW5RemT165B7SPq/VulTTJvkP3R8ciJqO8A8BEA9zPGjpb/vFvlcxmOYonjwkoKu4a82DPsxen5q9wfWcgZcSpoZ4JePJOHxcTgsLZfKl9ZHqDAlpeIRnM+BHaLGT6HRbdk4rnFBFw2M8YCTs0f2++0YuegB29MU+VHp9MwtOOcvwhAu3S1QZmLrSFXKGFHyI34Wh5Pnlps6X5asj5aqKVOlOd8SB+I2sNuMcNuMSmy5aVifWgk1IC+tdRSxYdH04qPag5sDeCnpxfBOVfkd4HQB+pMlMlUOXm4Y8CDsYATK6lc07W5Ult3E8nENiJqpdrHBUq1kUc09qgBsY1cn6qPc4tJ7NIhkSi4aVsQ0XQel1bSup2BaB8SapnMRtcAAON9LoyWP8bOr2aauo9soYR8sfHSAEFb1seaMrOoBT6HRRHrI5rOwWxiilSjyCXktetifcTSOSwlsrokEgUHxss+9TT51J0MCbVMwvEMTEzqdBNCPRdba+o+Kk0oTScTW7E+lI2ovQ6rItZHJJVH0GXT9GP4gEefCXqVRKKOEfWuQS/cNjNN0utwSKhlshjPIuSxw2I2YTTgANC8UDdbidFOeZ461kf7EXUsndOs4kMw4LUjkdW+jfxcpTRPP6E2mxj2bw2QUHc4JNQyWYhnMOSTBHrYL4S6OeujmYFMAGCzmGC3mJBsoTsxrtAaLoFPob2JkZR2cz4EIY8+m14mFxPw2C0YLf++6MWB8QBOz8exptBQLUJ7SKhlshjPYMgnNU/YLWb4nVaspJr7j9/MiFOBt0VvuJkyQHnnsCrT8JLWbs6HQK/diecWk9g56NG92uLA1iAKJY4Tc9T40qmQUMsknMhi0LceGfV7bFhJNVdJ0GxELd22+eUBxRJHMqu09WFRqOpDuzkfgvXBTNpWfkyGE7omEgU3jlPjS6dDQi2DbKGISCqH4SqhDrntWGnyo3Qro0c9dkvT1kdSwYFMAp/Dilyh1JbPyzmXImrNrQ/tJ+hFUjksJ3O6+tOCkMeO8T4XjTztYEioZSA+MgvrA5AWs640GaG10tbdyqhTEfn6FRhxKvC1USooiGcKKJa4ZttdBKKNXEvrQyQS9az4qObAeACvX44qtqWH0BYSahksxqWkYbvWh/CaRdmdHLwOS9OzPpScRS1QYnlAVKPt4xsROQUtI2o9hzHV4sDWAMKJbNO1/4QxIKGWgfA2Q+71iLrfY0c0nUOxiZ10iUweHrsF5ibaiT325j1qkfRTtupDLA9oXagrcz40FmpAqvzQUqjPLSbhtVuusMv05EB54wuV6XUmJNQyiFUEZl34Qh4bOJeqGOTSSm1zK9bHqhhx6lQ2mQi0Z33oMedDMODVtulFLAvQu+JDsHfEB5vFRAnFDoWEWgaRlCR81QIjPr4341NLcz6aF+pkrtDUNun1Dkhly/Oq77sVhFXUr0tEbde06mMynDREIlFgs5hw/ZifJul1KCTUMoilc7BZTHDZzJXL+ss2SDOVH63UNnsdFnAOpHLyI9nK0gBFk4nt700Ub2oiuacl0mAmbSLq5WQWkVTOMIlEwYGtAbw5u4pcoaT3UYgmIaGWgbQ1+8qRoZVutyYSilK3YHMRtccuCWQzJXrxTAGMSau8lPfZJmMAACAASURBVEJYH+1E1JFUFk6rGS4ZG9iVRss28nMGSyQKDowHkSuUWp6lTugHCbUMxCChaoT1EdEgohbfK5f4mpS0VHIGstNqhtnE2mojX0nmdImmAWkwE6BNid5keRiTkawPoGqSHvnUHQcJtQxi6avnU4ga5VgTVRCtJhOl75X/OPFMXtEaagBgTBpN2o71sZzKod9jb3xDFQh5y7XUGlR+nFtMwOewYNCrz3PdjNGAE8M+B/nUHQgJtQyi6dwVFR8AYDGb4LVbKhUWjWh2aYCgtYi6oGgiUdDu8oBIKqtLIhEABr1SmVw4rkFEHZaWBRil4qOaA+M0Sa8TIaGWQTR9tfUBAH6XFatpecKVyUtLA5otmWtl1Gk8k1e0NE/gc1jbKs9bSeZ0E2ox8VA0L6nJVDhpOH9acGA8gMuRtG47JInWIKFuQKnEa1ofABBwWWVbH61MzgOqlgc0k0xUeLuLwOe0VGrKm4VzXvao9bED+lw2WM1M9c48UfGxU4et43KgxpfOhIS6AfFMHiVeu5su4LTJFi7RPt5s1UdLHvVaXtHSPEHAZUNM5ieIjSSzBeSKJd0iapOJYdDrUD2iFonEXYPGjKivG/XDYmKUUOwwSKgbEE2LZperhc/fQkTdbKTrtlnAWLPWhzoedbCJ57sRPWuoBSN+BxZUjqinwvpvdamH02bG3hEfRdQdBgl1A0SLeE2P2info463OHrUZGLw2OS3kReKJSSzBVU8avEJopkuSYFYsqCX9QEAQ34HFlSOqMWMj+pJi0bjwHgAx2ZiTc2pIfSFhLoBlfkUNa0PKcKUMzqyVY9a+h75Qi28bDUi6oDLihJvbd5HJaLWyfoAgBGfFFGrOepzMmysGR+1ODAeQDpXrDTmEMaHhLoB9ayPgMuKYokjJWMXXSvbXQQehwXJrMzIXUzOU8GjFp8qYmvNJxQrcz50tD6G/Q6s5YuKrBTbjKlwErsMmkgUHNhKCcVOg4S6AfUj6rJwyUgoJtqYEe1toixufSCTCtZH+c0q2kJCUcxE0XoWdTViObFa9ofY6rLLoKV5gm39LvS5bXjtEiUUOwUS6gZE0zlYTKzm3Ax/WbjkVEIkMgWYGOCuGuwkl2asDzGQSenORECq+gCaG+0qWEnl4LVbYLc0//yVYsSvrlBPGmyry2YwxnBwWxCvXFzR+yiETEioGxBN5xHYMJBJECiLoZzuRDF/oxXvspm9iWpsdxEEK29MLQi1jnM+BJWIenVNlfs/FzZ2aV41t+7ox3RkDXMxdX4WhLKQUDcgmqrd7AKsR5hyI+pWEolAc5vINfGoW7E+UlldKz6AaqFWpytvajEBj91SidyNzK3b+wAAr1yI6HwSQg4k1A2IbtKVCFQPZmocYcYzhZbFsxnro7LdRQWP2ue0grFWPeqcrv40IA3PD3ls6lkf4SR2Dhq74kOwd8QHr8OCwyTUHQEJdQNi6fxVA5kEIrkmx/poZbuLwGu3IFsoyRr4Hs/ky1648kJtNjH4HNbWrI9UrjLDW0+G/Q71rI/FZEfYHoD0Wt4y0YfDF8in7gRIqBsQqRNRO6xm2C0mWU0vUrdgi0LdRBt5fE2a0KfkLOpqgi5r09ZHqcQRSeUqW3H0ZNjnwIIKE/SiqRyWk1nDdiTW4tD2PpxfSmm6S5JoDRLqOnAuDWQK1FnGGpApXK2MOBV4HPK3vKyuqTM5T+B32Zqu+ohn8iiWuO7WByD51GpE1FNLUiJxp8FL86ohn7pzaCjUjLEvM8bCjLETWhzISCSzBeSLHH2bWB9Aua1ahkedUCSibizUm41kVYpWImoxUlPvqg9AKtGLpvOKr+QSXX6dYn0AwHVjfrhsZrxC9ofhkRNR/y2AB1U+hyERglQvovbLEK5WlwYIhFDHZXjhjT4BtEuwhYhaDOsXw/v1ZNjvBADFx51OLibhtpkxFnAqer9qYjWbcPO2ICUUO4CGQs05fx5AT76S9QYyCQJOa8NkYipXRIm31j4uPYb0+HKSlrG1fM12d6VoZhCVIFz2QAcNMKhoS1AS0tmosvbHVAdVfFRz6/Y+nFlIINLEkmZCexTzqBljjzLGjjDGjiwtLSl1t7oifnnrWR9+Z+OIup2BTMB6dYmcEaPRVK7SiKMGQZcNiWwB+WLjChRBOCFFrwMG2CFYEepYWtH7PbeYMHxHYi3u2BkCALw0tazzSYh6KCbUnPPPc84Pcs4PDgwMKHW3uiLH+gi4GkfU4vpAi5GuX2YHZLHEEc8U1LU+3PLb5gVLiSwcVlPNNnytGfY5YDYxzCgYUa+m8wgnsh3lTwtu2BKAz2HBi5Mk1EaGqj7qUImo6wq1DWv5Yt3kVEXwW4x0XTYzrGbWUBzbfUOQg3gTWG1igl44kcWg12EIW8BiNmHY51BUqM8sxAEAe4Y7L6I2mxju2BnCC5NLqo5/JdqDhLoOsXQOjNVvxxbRbr1EnxBYf4sCyhiTvOEGEbUcT71dxJtNM92J4XgWgwawPQRjQSdmospZH2fLFR/XDvsUu08tuXNXCHOrGZxfTul9FGIT5JTnfR3AywD2MMZmGGOPqH8sYxBN5+F3WmGu0zwixz8W0Wc7loQk1PWj2HWrRr2IWtRCi0UAcggnMobwpwVbgk5Fk4mn5xMIuKyG3upSj7t3SVblC+e6I7fUjcip+niYcz7CObdyzrdwzr+kxcGMQCSdq2t7AOsVGdE6WfN2rQ9AEvlGEbVo7VbToxaCK2qj5bCUMFZEvSXowkI8I6slXw5nFuK4dthrCGunFbb2ubCt34UXyKc2LD1nfbxyIYLP/uS0rIaHaCrXMDqVE1HH1vKwmhlcLcyiFsipLqm3jUYpmo2oM/ki4pkCBn3611ALtgScKHEosui2VOI4t5DoWNtDcNeuEH5+fkWxNy9CWXpKqPPFEh7521fxN8+dx9cOX254+0gqh74G8ynE5pd6g4pi6Tz8TltbEZecem0tImqr2YSAyyo7ohZzJIxmfQDAjAIlejPRNaRyRVzbgYnEau7aNYBUrogjF3uyZcLw9JRQv34pikR5XsZPTy82vH00nWu4jFVEr5FUfY+6Xd/YJ6PRJJaWJuepXQbX77bJFmoj1VALxoRQK+BTd3LFRzV37gzBZjHhSRn/Lwjt6SmhfnN2FQDw4NuGcWJ2tW45EufSxLdauxKrcVrNsFlMDSPqdptQAi4rEtkCCnUaTaLl9nG1JucJQh67bOtjvX3cOEI94neCMaWEOgHG0FFT82rhtltw584Qnjy1SGV6BqSnhPrkXBzDPgfu2h1CPFOo+x9VzkAmQCqdC7qsdedfrK7l295hWCkDrDOYKbaWV7XiQxDy2uVbH0njzPkQ2CzlWupI+9bHmYU4tvW54DZAM0+7vHPfEGaiazizkND7KMQGekqoT83FsW/Uh30jUuLn1Hx809uut483jgSlQUX166hbraEWBGTsK4yl1W0fF4SasT7iWZhNzBAjTquZ6Hfjwkr7dcNnFhIdb3sI3rF3EIwB/3SS7A+j0TNCXSpxXFhOYdegBzsGpFbfS3X+o8qZ8yEIumx1y/NW1/KVMr5WkTOYKZpSd8SpIOSxI54pIFtoXDkTTmTQ77bVrUXXg4mQGxfbbPDI5Iu4uJzq+IoPwaDXgRu3BvDk6QW9j0JsoGeEeiGeQa5Ywni/C36nFT6HBdORza2PpiJq9+bWR75YQjJbUCSZCNQvA5SqVNQXarGkVs7EtYV4FsMGXPa6I+RGNJ1vaa2Y4NxiAiWOjq/4qOYX3jaME7NxXF5RdmgV0R49I9SXy37keJ8LgFTkP12njVjOnA9BwGXbtMZZqfkb4vs3a1XnnGu26VvsPlxONBa5+diaIbdyT4TcAIALbUTVIjl93ZhfkTMZgYduGAEA/ODYrM4nIarpXaEOujBdJ5lUEWoZW0mCLitia3mUSldnyytzPhRKJm5mscQzUvJTiwWyIZndiZxzzMXWMGrAYfrbFRDqE7Or8DutlbrsbmBL0IVDE3343tE5qv4wEL0j1CtpmE2sIhpb+5yYia5t+ssYSedgM5vgltFNGHTZUCzxmquylJjzAUgNL4xtPgxppSyaIS0iarc8oY5nCkjlihj1G0/IxvtcMDG05VO/ObuK68f8Hds6vhnvvXEUU+Fk3WQ7oS29I9SRNEYDDljN0lPe2udCtlDadANzJCn5vXL+E4oEXi2fWok5H4A0njPo2rzaYrlc16zFXsKQV3qMpQZCPReTcgAjAeNZHzaLCVuCrpYnxmULRZxdSHSV7SF4z/UjsJgYvn90Tu+jEGV6SqiF7QFI1geATX3qaLpxs4tADNOvJdTCQlGiGqPfbdu00URE1P0ykp/t4rJZ4LVbKs0smzFf3vZtROsDKFd+tFiid24hiXyR4/ouFOqg24Z79wzgH9+YbWqTD6EePSPUM9F0RZyBqnkPmzS9rKQat48LhK1RK6EoIl0RhbZDv8eGldQmEXX5DUELjxoAhv2OihBvxlxMah83ovUBSJUfF5ZSLXmxIpHYjUINAA8fGsdSIosnT1FNtRHoCaHO5ItYTuauSPqIKG82VltsojLaxwWiMqRWudpKMgun1QyXrf3Otf46rdsiotaqsWTY78BCg4h6LrYGi4kZas5HNdtDbqRyxU3tr3q8WU4kbu0z5ptQu9y7ZxBjASf+7uVLeh+FAND5fa8yEFHzlqqI2m23IOCybjpAfiUpP6Ku51EvJ7OKRNNA/Y7A5WQWQZcVFrM2770jfgfOLdYfND+/msFQeUehEdlZ3nF4bjHZ9BjWE7OruG7M13WJRIHZxPDh27bhDx8/g0kDL+7N5IuIpnNYSeYQSeUQTeeQzhWRyhakv3MFpLPS38USB4M09sFU3prU57aiz23HRMiFnYMeDHjshnxNe0SoJR96YxnVWMBZSXhVs5YrIpEtyI4EfU4LrGZWsTmqkSwUZSLK/nJHYK5Qgs1ypSCvJHOa1FALhv1OhBNZ5IulSoJ2I1JpnvESiQLRqHJmIY47d4Vkf59IJP7anRMqncwY/PLBLfjzp87hc8+dx//3y/t1OwfnHJdW0jg+u4qTs6u4tJLGdDSN6Ui67uwbALBbTHDbLXBazbCYGTgHODiKRWkRdDJ75ff7nVbcuDWAQ9v7cMtEHw6MBzb9/daSHhFqSYzHagh1rWSSGM0pd+IbYwwDHnvNj9BLiewVkXw7hKo6Ajd2+y0lspr504AUUXMuPe5mycK51TUc2BrU7EzN0u+xI+SxNz2E6MRsHLliCQe2BlQ6mTHo99jxoUPb8JWXL+IT79iF8X5lfo/lEEnl8NTpRTx7dgkvTi1XGsdsZhO29jmxtc+Fm8aDGPY70Oe2Ieiyod9jQ9BlhcduhctuhstqbvgJM1soYiWZw/mlFKbCCZxdTODIxSj+6ImzAACvw4L79gzigX1DuG/PALwO9Wfp1KInhHo2tgarmV01wW004MRLU8vgnF/xcSdcFtyhJj4OD3jtNcvVlpM5HBhX5j+0KL1bTl7dlr0Qz+CWiT5FHkcO4vHnVzM1hTpXKGE2uoZfunFMszO1wt4RL842KdSvXZKG69+8Tbuft1587J4d+PvDl/BXz0zhD99/g6qPVSiW8OzZJXzrtWk8dTqMQolj0GvHu/YN4eZtQVy/xY/dQ15FI1y7xYzRgBOjAecVn6oiqRxeubCCp06H8fSZMH5wbA52iwkP7BvC/3HjGO7ZM6BppN0TQj0TlbrjNnqlW4JOpHJFaWhSVfncYrwcUTexrHTAa7+qgqRU4oiksopZHyJiXtmQtCyVOBbjGU1naoi28M3WWc3G1lDiwLZ+t2ZnaoU9Q1783c8voVAsyfb3j1yMYlu/y7BJUiUZ8jnwoUPj+OrLF/Ev7pjA3hHlB1ClcwV889VpfOH585hbzSDkseFf3D6BXzowhreN6pMH6HPb8OB1I3jwuhEUSxyvX47ih8fm8MNjc/jR8Xn0uW147/5RfPDQVk2GcvWIUKdrtvmOVVV+VAv1+rD7ZiJqB45Ox664LJrOocSVK5kTgr/RYllJ5ZAvck1naoz4RXlj7Tp0MZlwm4Yfl1vh2hEfsoUSLq6kK8nFenDO8dqlKO7ZM6DB6YzBbz2wC98/OotP/+AkvvnobYoJZ65Qwldfvoi/fvYtRFI5HJrow6ff+zbcf+2gIXxhgdnEcMuE5Fn//kP78NzZJfzj0Vl87ZXL+NufXcSB8QA+dGgcD90wCmcbe1Hr0SNCvYb79wxedXmlRC+6hreNrtfDhhNZWM2sqSWxA147VlK5KyIzEfkqleSr2A0bEqAiqh3WcIGs32lFwGXFpU3mpVwqT18zvFCXE4qn5+OyhPriShorqRwO9oDtIQi4bPidX9iD//iPJ/Dt12bwgYNb27o/zjmeOLmIz/7kNC6tpHH37gH85v07cVBD665VrGbJ/nhg3xCiqRy+8/oMvvbKZfzut4/jM4+dwv95YAz/z3v2wm5RVrC7XqgzealOdmMiEVhPLm6spQ4nMhj0OpqKHAa8dnAueVui1EtEvkrN33BYzeh32zC3wW4QjScjGjeWbOtzbToO8+JKCi6bGQMaVqK0wu4hL+wWE45Nx/CL+0cb3l4sfz04YdwkqRp88JZxfP+NOXz6BydxcKKvMtSqWU7MruI/PXYKhy9EsGvQg6/8+iHcs7szP50E3Tb8xl078Mid2/HKhQi+9splvDm7CpsKnwa6XqhF+V0t66PfbYPdYrqqRC8czzbtPwpBCieyFaGeLwuqkpbESODqjsCFsqeu9dznbf1uvDEdrXndpRWpZd+INanV2CwmXD/mx+uXaz+PjRy+EIHfacXOgcbRdzdhNjH82QdvxLv/4gU88pVX8a2Pvb2pT4oLqxn80RNn8d03ZhB02fCff+k6fPCWrZrV/asJYwy37ujHrTv6pVptFX7nO/+nBKnE5txiomYrcK1mFwFjDGMBZ82IeqiJRCKwnnis9o+FRaGkgI76r679nl/NwGpmsht0lGJbvwtzsUzNeRDnl5ItR11ac2A8gBOz8YYbazjneGFyCXfuDKm+QNiIjAac+MJHD2I2uoYPf+mVmj0IG4ln8viTJ8/h3j9+Bj88NodH79qBZ3/3Xnz4tm1dIdIbUau5qyt+Ur/9zaN4158+j79+9q2rrrvYIKk1FnRe0Z3IOcdMdA1jgea8VVFzLaJbAJiPZ9DntsFhVc6vGg04MR/bYH3E1jDodWguHuN9LhRL/KruznSugEuRdMfsErxpPIhcsYSTc/XHek6Gk1iMZ3FXE80x3cYtE334wkcPYjqSxnv/x4v4zmszKNR4o76wnMJ//fFp3P7Zp/EXT03igb1DeOqT9+BT794Ln061yJ1Mx1sfS4ksfvymtOPtf710AR+7e8cV79RT4SS8dsumzStjASdOV83djaSkFtRmZzgMl1ulq0VLje0mowEHEtkC4pl85Rf+0obJgFohSu8urqQqG1MAYHIxCd5BK6pu2ib5za9fiuKm8c295+fPSS3zd3Wop6oUd+8ewHf/ze343W8dwye/dQyf/ckZHNoeRL/bjthaHqfmVvHWUgpmE8O7rx/Bo3ftwPVbunN4lVZ0fET9XPk/z8fvuwbLyRwOX4hccf1UOIlrBj2b+kajASeWkzlk8tLH3npWST0sZhNG/I4rxqbOr2YUF2qRMKyOqi+tpK8QSq3YPST5tBsbRs4uSl/v6ZClr0M+B7b2OfHz85G6t3vu3BKuGXBXyjp7md1DXvzjv7kDn/vwzbh1ex9OzcXxozfn8cblKMb7XPj9h/bhxd+7D3/58AESaQXo+Ij62bNhDHrt+Ph9O/GFFy7gqdNh3LFz/aPpW0tJ3Llz8whIRKKXVqSP6kJoW5mKtiXorAg95xyXI2nctqO/6fupx1hwvX55z7AXq2t5RFI5TOhQBhdw2TAWcF5lGZxdSMBhNekS5bfK3bsG8L03ZmvOUQGA1XQePz+/gl+7Y7sOpzMmJhPDg9cN48HrhvU+StfT0RF1oVjC8+eWcM/uAbhsFty6vQ/PnQtXro9n8liMZ+vWx4rrpsJJAOv1v1tbmM+xNeiqNIAsxrNI54q4ZkDZSPea0JXnFauk9IioAWDviO+qlU2n5uLYM+Q17NS8Wty7ZxCpXBFHLtWOqp88vYh8kePd149ofDKC6HChfmM6hnimgPuulZpZ7t0ziLeWUpWltULM6onlNQMeMAZMhqWP6+cWExgLOOG2N/9hY1u/C4vxLJLZAs4vS4+9Q+EyLr/LikGvvfLchM0gp1lDDfaN+nB+KYm1nGQdFYolHJuJ4UAdr9eI3H5NP2xmE/7pZO1B+T84NoexgBP76WM8oQMdLdTPnAnDbGKVYSqicF741sfKLd31PDKnzYytQRcmF8vCt5BouVpBeLJnFxI4vyRFumqUqO0c9GCyLNSn5uJw2czYrtNMjX0jPpQ4cHJO2nhyZiGBdK5YSdB1Cm67BQ/sG8QPj81dVW54eSWNFyaX8P6btxi+LpzoTjpGqEsljtUNq65+enoRB7cFK9UP1wy4sSXorAj165djGPE7GnbsXT/mx9HpGHKFEt5aSmJ3i0PS946szzc+MbsKn8OiSlv33hEfTs9Ldb8n51axb8SnW13voe19YAx4aWoFAPD8pPSzP9QB7cAb+ec3bcFKKoenToevuPyrL1+EiTE8fGhcn4MRPY8soWaMPcgYO8sYm2KM/Qe1D7WRQrGE3/jqEez/zD/hb56TaqXPLMRxbjGJ99yw7hkyxnDP7gH8bGoZmXwRh8+vyIrsDk4EMRtbw2PH55AvctzY4pzhsYATXocFx6dXceRSFDdvC6oioIe29yFbKOHlt1ZwbGa15fMqQZ/bhuvH/HjmrCRuT55axPVjfs27JJXg7t0DGO9z4c+fmkSpJDVPzUTT+OrPL+F9N4525HMiuoOGQs0YMwP4KwD/DMA+AA8zxvapfbBwPIPpSBqlEsf/+8OTePpMGFv7nPjsT87g+0dn8fnnz8NmMV2V3Ln/Wikp9JnHTiGcyOKfychI37pdqsz4zGOnYDEx3LGztUoNxhju3j2Abx6ZxlQ4qdqQmUMTUhT7nx47hVyhhHfsHVLlceTy0A0jODodwzdfvYw3Lsfw0A2dmXCzmk345Lt24/R8HF968QIy+SJ+7zvHwQD8zrv26H08ooeRkzE7BGCKc34eABhj3wDwPgCnlD7MHf/taZQ4h9nEKmVudosJ2UIJH7t7B/7vd+3GR774Cj7xjaMApKHmGwce3btnELuHPPja4cvoc9vwjmsbi9jeES+uHfbizEIC91872NYWh4euH8GPjs8DgGplS0G3DfftGcTTZ8IY8Ttwi84Dgv6vg+P4y6en8HvfeRMBlxUfurVzLYJfvGEUP35zHv/lx6fxZz89h1SuiD/+wP5Nt9gQhBawWvMxrrgBY+8H8CDn/DfKX38EwK2c83+74XaPAngUAMbHx2++dKn57cV/8MOTSGULyBZK2D3khd9pxan5OG7cGsAHyomcRCaPL7xwAVYTw8fuuaZmzevkYgJffukC3n/zFtlbOKbCCXzvjTn8ym3jbU2hK5U4vvjieQz7nXivjGlsrbKwmsGXXjyPh24YxX4DrIQ6cjGC/31kGg8fGu+4io+NiDnJ5xYT+MX9o7hrV293IhLawBh7jXN+sOZ1Sgl1NQcPHuRHjhxp48gEQRC9RT2hlpNMnAVQPSl8S/kygiAIQgPkCPWrAHYxxrYzxmwAPgjgB+oeiyAIghA0TCZyzguMsX8L4AkAZgBf5pyfVP1kBEEQBACZQ5k45z8G8GOVz0IQBEHUoGM6EwmCIHoVEmqCIAiDQ0JNEARhcEioCYIgDE7DhpeW7pSxJQDNtyZKhAAsK3icToCec29Az7n7aef5buOc12yDVUWo24ExdmSz7pxuhZ5zb0DPuftR6/mS9UEQBGFwSKgJgiAMjhGF+vN6H0AH6Dn3BvScux9Vnq/hPGqCIAjiSowYURMEQRBVkFATBEEYHMMItd4LdNWCMbaVMfYMY+wUY+wkY+wT5cv7GGNPMsYmy38Hy5czxthflH8OxxljN+n7DFqHMWZmjL3BGHus/PV2xtjh8nP7ZnlsLhhj9vLXU+XrJ/Q8d6swxgKMsW8zxs4wxk4zxt7e7a8zY+y3y7/XJxhjX2eMObrtdWaMfZkxFmaMnai6rOnXlTH2q+XbTzLGfrWZMxhCqPVaoKsRBQCf5JzvA3AbgI+Xn9t/APAU53wXgKfKXwPSz2BX+c+jAP6n9kdWjE8AOF319R8C+FPO+U4AUQCPlC9/BEC0fPmflm/Xifw5gMc559cC2A/puXft68wYGwPwmwAOcs6vgzQG+YPovtf5bwE8uOGypl5XxlgfgE8DuBXSHtpPC3GXBedc9z8A3g7giaqvPwXgU3qfS6Xn+n0A7wRwFsBI+bIRAGfL//4bAA9X3b5yu076A2kT0FMA7gfwGAAGqWPLsvE1hzTr/O3lf1vKt2N6P4cmn68fwIWN5+7m1xnAGIBpAH3l1+0xAL/Qja8zgAkAJ1p9XQE8DOBvqi6/4naN/hgiosb6Cy6YKV/WVZQ/6h0AcBjAEOd8vnzVAgCxLr1bfhZ/BuDfAyiVv+4HEOOcF8pfVz+vynMuX79avn0nsR3AEoD/VbZ7vsgYc6OLX2fO+SyAPwZwGcA8pNftNXT36yxo9nVt6/U2ilB3PYwxD4DvAPgtznm8+jouvcV2TZ0kY+whAGHO+Wt6n0VDLABuAvA/OecHAKSw/nEYQFe+zkEA74P0JjUKwI2rLYKuR4vX1ShC3dULdBljVkgi/Q+c8++WL15kjI2Urx8BEC5f3g0/izsAvJcxdhHANyDZH38OIMAYE1uFqp9X5TmXr/cDWNHywAowA2CGc364/PW3IQl3N7/ODwC4wDlf4pznAXwX0mvfza+zoNnXta3X2yhC3bULdBljDMCXAJzmIDf4zwAAAUFJREFUnP9J1VU/ACAyv78KybsWl3+0nD2+DcBq1UesjoBz/inO+RbO+QSk1/JpzvmvAHgGwPvLN9v4nMXP4v3l23dU5Mk5XwAwzRjbU77oHQBOoYtfZ0iWx22MMVf591w85659nato9nV9AsC7GGPB8ieRd5Uvk4feJn2Vuf5uAOcAvAXgP+p9HgWf152QPhYdB3C0/OfdkLy5pwBMAvgpgL7y7RmkCpi3ALwJKaOu+/No4/nfC+Cx8r93AHgFwBSAbwGwly93lL+eKl+/Q+9zt/hcbwRwpPxafw9AsNtfZwB/AOAMgBMA/g6AvdteZwBfh+TB5yF9cnqkldcVwK+Xn/sUgF9r5gzUQk4QBGFwjGJ9EARBEJtAQk0QBGFwSKgJgiAMDgk1QRCEwSGhJgiCMDgk1ARBEAaHhJogCMLg/P/vDR8hpE8QGAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rFk60dZEiqFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}