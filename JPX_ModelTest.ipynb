{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JPX_ModelTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "\n",
        "!rm -r sample_data\n",
        "!kaggle competitions download -c jpx-tokyo-stock-exchange-prediction\n",
        "!unzip ./jpx-tokyo-stock-exchange-prediction.zip -d jpx-tokyo-stock-exchange-prediction"
      ],
      "metadata": {
        "id": "GHRbYCxEWZIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "A3HcDY9Ws1s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "5rAq4kJLep0K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSDataset(Dataset):\n",
        "  def __init__(self, df, seq_len=128, padding_token=0, vec_dates=True, normalize=True):\n",
        "    self.vec_dates = vec_dates\n",
        "    \n",
        "    self.df = df\n",
        "    self.indices = []\n",
        "    self.seq_len = seq_len\n",
        "    self.normalize = normalize\n",
        "    self.padding_token = padding_token\n",
        "    \n",
        "    #Creating indices\n",
        "    start = 0\n",
        "    for _ in range(-(len(self.df) // -self.seq_len)):\n",
        "      self.indices.append((start, start+self.seq_len))\n",
        "      start+=self.seq_len\n",
        "    \n",
        "    #fixing non-perfect intervals, --in place\n",
        "    idx = 0\n",
        "    while idx<len(self.indices):\n",
        "      start, end = self.indices[idx]\n",
        "      intervals = self.df[start:end]['SecuritiesCode'].value_counts(sort=False).values\n",
        "      if len(intervals) != 1:\n",
        "        self.indices = self.indices[:idx] + [(start, start+intervals[0]), (start+intervals[0], end)] + self.indices[idx+1:]\n",
        "        idx+=1\n",
        "      idx+=1\n",
        "    \n",
        "    #Getting normalizing values for each stock\n",
        "    if normalize:\n",
        "      self.norm_values = {}\n",
        "      stock_list = dframe.SecuritiesCode.unique()\n",
        "      for stock in stock_list:\n",
        "        local_series = dframe_1[dframe_1['SecuritiesCode'] == stock].Close\n",
        "        self.norm_values[stock] = (local_series.max(), local_series.min())\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    start, end = self.indices[idx]\n",
        "    seq_df = self.df[start:end]\n",
        "    \n",
        "    target = (seq_df['Target'].values[-1])\n",
        "\n",
        "    sequence = np.expand_dims(seq_df['Close'].values, 1)\n",
        "    \n",
        "    #Normalizing\n",
        "    if self.normalize:\n",
        "      stock_max, stock_min = self.norm_values[seq_df['SecuritiesCode'].iloc[0]]\n",
        "      sequence = (sequence - stock_min)/(stock_max - stock_min)\n",
        "    \n",
        "    #Padding\n",
        "    if sequence.shape[0] != self.seq_len:\n",
        "     sequence = np.pad(sequence, pad_width=[(self.seq_len-sequence.shape[0], 0), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "\n",
        "    #careful here padding_mask shape shouldn't be the same as sequence's, it works now bc we're using only one feature\n",
        "    padding_mask = (sequence == self.padding_token)\n",
        "    if self.vec_dates:\n",
        "      date_vec = np.concatenate([np.expand_dims(seq_df['Date'].dt.year.values, 1), \n",
        "                                 np.expand_dims(seq_df['Date'].dt.month.values, 1), \n",
        "                                 np.expand_dims(seq_df['Date'].dt.day.values, 1)], \n",
        "                                axis=1)\n",
        "      date_vec = np.pad(date_vec, pad_width=[(self.seq_len-date_vec.shape[0], 0), (0, 0)], constant_values=self.padding_token, mode='constant')\n",
        "      \n",
        "      return {'sequence':sequence,\n",
        "              'date':date_vec,\n",
        "              'mask':padding_mask,\n",
        "              'target':target}\n",
        "    else:\n",
        "      \n",
        "      return {'sequence':sequence,\n",
        "              'mask':padding_mask,\n",
        "              'target':target}"
      ],
      "metadata": {
        "id": "jrgh8BkAdvzY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class time2vec(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    self.w_linear = nn.Parameter(data=torch.rand(in_features, 1))\n",
        "    self.b_linear = nn.Parameter(data=torch.rand(1))\n",
        "    self.w_function = nn.Parameter(data=torch.rand(in_features, out_features-1))\n",
        "    self.b_function = nn.Parameter(data=torch.rand(out_features-1))\n",
        "\n",
        "    #maybe a bit more straightforward\n",
        "    #self.linear_params = nn.Linear(in_features, 1, bias=True)\n",
        "    #self.function_params = nn.Linear(in_features, out_features-1, bias=True)\n",
        "\n",
        "    #initialize params?\n",
        "    #nn.init.kaiming_normal_(self.w_linear)\n",
        "    #nn.init.kaiming_normal_(self.b_linear)\n",
        "    #nn.init.kaiming_normal_(self.w_function)\n",
        "    #nn.init.kaiming_normal_(self.b_function)\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear_out = torch.matmul(x, self.w_linear)+self.b_linear\n",
        "    func_out = torch.sin(torch.matmul(x, self.w_function)+self.b_function)\n",
        "    return torch.concat((linear_out, func_out), dim=-1)"
      ],
      "metadata": {
        "id": "YtoKQLCyJSC4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTransformer(nn.Module):\n",
        "  def __init__(self, in_features, time_features=7, mlp_dim=1024, enc_layers=2, enc_heads=2):\n",
        "    super().__init__()\n",
        "    self.time2vec = time2vec(in_features, time_features)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=in_features+time_features, nhead=enc_heads, \n",
        "                                                    dropout=0, activation=F.gelu, batch_first=True, \n",
        "                                                    norm_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=enc_layers)\n",
        "\n",
        "    self.mlp = nn.Linear(in_features+time_features, mlp_dim)\n",
        "    self.regressor = nn.Linear(mlp_dim, 1)\n",
        "\n",
        "  def forward(self, seq, mask):\n",
        "    time_embeddings = self.time2vec(seq)  \n",
        "    x = torch.concat((seq, time_embeddings), dim=-1)\n",
        "    x = self.encoder(src=x, src_key_padding_mask=mask)\n",
        "\n",
        "    x = F.relu(self.mlp(x))\n",
        "    x = self.regressor(x)\n",
        "\n",
        "    return x[:, -1, :] #returning only last seq element"
      ],
      "metadata": {
        "id": "hEAvaGbUJVZw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSTransformer_VecDates(nn.Module):\n",
        "  def __init__(self, in_features, time_features=7, mlp_dim=1024, enc_layers=2, enc_heads=2):\n",
        "    super().__init__()\n",
        "    self.time2vec = time2vec(3, time_features)\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=in_features+time_features, nhead=enc_heads, \n",
        "                                                    dropout=0, activation=F.gelu, batch_first=True, \n",
        "                                                    norm_first=True)\n",
        "    self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=enc_layers)\n",
        "\n",
        "    self.mlp = nn.Linear(in_features+time_features, mlp_dim)\n",
        "    self.regressor = nn.Linear(mlp_dim, 1)\n",
        "\n",
        "  def forward(self, seq, date_vec, mask):\n",
        "\n",
        "    time_embeddings = self.time2vec(date_vec)\n",
        "    x = torch.concat((seq, time_embeddings), dim=-1)\n",
        "    x = self.encoder(src=x, src_key_padding_mask=mask)\n",
        "\n",
        "    x = F.relu(self.mlp(x))\n",
        "    x = self.regressor(x)\n",
        "\n",
        "    return x[:, -1, :] #returning only last seq element"
      ],
      "metadata": {
        "id": "-E02oGLrhHYu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128\n",
        "\n",
        "padding_token = 0.0\n",
        "missing_token = -1.0\n",
        "\n",
        "\n",
        "dframe = pd.read_csv('jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv', parse_dates=['Date'])\n",
        "\n",
        "stock_list = dframe.SecuritiesCode.unique()\n",
        "dframe_1 = dframe.drop(['Open', 'High', 'Low', 'Volume', 'RowId', 'AdjustmentFactor', 'ExpectedDividend', 'SupervisionFlag'], axis=1)\n",
        "dframe_1 = dframe_1[~dframe_1['Close'].isnull()] #Getting rid of null values for this experiment\n",
        "dframe_1 = dframe_1.sort_values(['SecuritiesCode', 'Date'], ascending=[True, True]).reset_index(drop=True)\n",
        "\n",
        "dset = TSDataset(dframe_1, seq_len=128, vec_dates=True, normalize=True)\n",
        "dloader = DataLoader(dset, batch_size=128, shuffle=True, num_workers=1)"
      ],
      "metadata": {
        "id": "WGnujAZM0RNZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project='TSTransformer_Test')\n",
        "wandb.config={'learning_rate': 1e-4,\n",
        "              'epochs':5,\n",
        "              'batch_size':128}\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "QAnK1K1AtZDB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TSTransformer_VecDates(in_features=1, time_features=7).to(device)\n",
        "wandb.watch(model)\n",
        "\n",
        "lr = 1e-4\n",
        "epochs = 5\n",
        "crit = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "running_loss = []\n",
        "for _ in range(epochs):\n",
        "  for _, batch in enumerate(dloader):\n",
        "    seq, date, mask, target = batch['sequence'].to(device), batch['date'].to(device), batch['mask'].to(device), batch['target'].to(device)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    out = model(seq.float(), date.float(), mask.squeeze(-1).float())\n",
        "    loss = crit(out.squeeze(-1), target.float())\n",
        "    \n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    \n",
        "    wandb.log({'loss': loss})\n",
        "    running_loss.append(loss.item())"
      ],
      "metadata": {
        "id": "rfL2NbgOuG05"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.squeeze(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FiAtJQ4vjwj",
        "outputId": "5bb4e52f-8ff3-4157-aa08-86b790d1ac1f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0359, -0.0789,  0.0096,  0.0549,  0.0800, -0.0903,  0.0997,  0.0520,\n",
              "        -0.0604,  0.0010, -0.0508, -0.0192, -0.0420, -0.0677, -0.0727, -0.0129,\n",
              "         0.0014, -0.0856,  0.0680,  0.0275, -0.0184, -0.0192, -0.0032, -0.1767,\n",
              "         0.0130, -0.0544,  0.0218, -0.0156,  0.0273, -0.1254,  0.0143, -0.0763,\n",
              "        -0.0121, -0.1134, -0.0624, -0.0720,  0.0016, -0.0235, -0.0270, -0.0370,\n",
              "        -0.0714,  0.0058,  0.0135,  0.0694,  0.0078, -0.0067], device='cuda:0',\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cbg7qgEvgX0",
        "outputId": "a5bf131c-dc18-4d08-8402-e2e2e1ad5514"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0117, -0.0125, -0.0114, -0.0038, -0.0248, -0.0023, -0.0421, -0.0057,\n",
              "         0.0076,  0.0322, -0.0071,  0.0196, -0.0095,  0.0262, -0.0224,  0.0255,\n",
              "         0.0062,  0.0065,  0.0016,  0.0061, -0.0120, -0.0082,  0.0090, -0.0110,\n",
              "         0.0194,  0.0233,  0.0447,  0.0140, -0.0057, -0.0079, -0.0053, -0.0059,\n",
              "        -0.0107,  0.0169,  0.0000, -0.0067,  0.0326,  0.0048,  0.0252,  0.0180,\n",
              "         0.0050,  0.0429, -0.0091,  0.0095, -0.0377,  0.0045], device='cuda:0',\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wjNFzzhgvsai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}